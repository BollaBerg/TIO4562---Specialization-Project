\section*{Abstract}
Artificial intelligence (AI) is everywhere, and has great potential for doing good, through its autonomous and scalable nature. However, several cases have shown that this nature also brings with it a potential for negative results, which, due to the vast reach of AI technology, can have large consequences for society. To avoid these negative results, AI systems should be developed to be responsible.

This paper reviews 54 papers on responsible AI, with the goal of creating an understanding of how AI systems can be designed to be responsible. This review results in a set of four core principles -- Autonomy, Beneficence, Non-maleficence and Justice -- that create responsible value by themselves, as well as seven instrumental principles for responsible AI -- Transparency, Accountability, Trust, Sustainability, Privacy and Others -- that create responsible value by facilitating for the core principles. Additionally, the reviewed literature included three clusters of antecedents for responsible AI, as well as three clusters of business advantages that can be gained by adopting responsible AI practices.

The review also uncovered a gap between abstract principles and actual responsible AI development. To help bridge this, this paper introduces a project-level framework for responsible AI development, the EPNIS framework. This framework is compared to existing frameworks for responsible AI, to fit it into the current ecosystem of responsible AI methods. Finally, the results of the review are used to point out future research that is needed within the field of responsible AI

\newpage
\section*{Oppsummering}
Kunstig intelligens (KI) er over alt, og har stort potensiale for gode resultater, via dets autonome og skalerbare natur. Flere hendelser har likevel vist at denne naturen også har potensiale for negative resultater, noe som, grunnet den store rekkevidden til kunstig intelligens, can ha store konsekvenser for samfunnet. For å unngå disse negative resultatene bør KI-systemer utvikles på en ansvarlig (engelsk: \textit{responsible}) måte.

Denne oppgaven analyserer 54 dokumenter om ansvarlig kunstig intelligens, med mål om å skape en forståelse for hvordan KI-systemer kan utvikles på en ansvarlig måte. Analysen resulterer i fire kjerneprinsipper for ansvarlig kunstig intelligens -- Autonomi (\textit{Autonomy}), Velgjørighet (\textit{Beneficence}), Ikke-skade (\textit{Non-maleficence}) og Rettferdighet (\textit{Justice}) -- som skaper ansvarsverdi på egenhånd, samt syv instrumentelle prinsipper -- Åpenhet (\textit{Transparency}), Ansvarlighet (\textit{Accountability}), Pålitelighet (\textit{Trust}), Bærekraftighet (\textit{Sustainability}), Rett til privatliv (\textit{Privacy}) og Andre (\textit{Others}) -- som skaper verdi gjennom å støtte og fasilitere for kjerneprinsippene. I tillegg finner analysen tre grupper med årsaker (\textit{antecedents}) til at selskaper utvikler ansvarlig kunstig intelligens, samt tre grupper med fordeler (\textit{business advantages}) bedrifter kan oppnå ved å bruke metoder for ansvarlig kunstig intelligens.

Analysen oppdaget også en avstand mellom abstrakte prinsipper og faktisk implementasjon av ansvarlig kunstig intelligens. For å minimere denne avstanden introduserer dette papiret et rammeverk for ansvarlig KI-utvikling på prosjektnivå, EPNIS-rammeverket. Dette sammenlignes med eksisterende rammeverk for ansvarlig kunstig intelligens, for å finne dets plass i det nåværende økosystemet for ansvarlig kunstig intelligens. Avslutningsvis brukes resultatene av analysen til å peke ut fremtidig forskning som behøves innenfor ansvarlig KI-feltet.

\newpage