\section{Discussion}
\label{sec:Discussion}
The aim of this paper was to create an understanding of how AI systems can be designed to be responsible, through a systematic literature review. The results of the review, as well as answers to RQ1, RQ2 and RQ3 are presented in \autoref{sec:Results}. This section will attempt to connect these results and answers to the overarching aim of the paper, and use them to create a framework for responsible AI development. In addition, some observations are discussed, alongside limitations of this study -- and potential for future research.

\subsection{Developing responsible AI systems}
Most of the reviewed papers agree that AI systems can be made responsible by following a set of ethical principles. Following the categorization given by \textcite{Ryan_2021}, this paper found 11 groups of principles for responsible AI. By adapting the notion of core principles used by \textcite{Canca_2020} and \textcite{Floridi_2018}, these were then distributed in two categories, resulting in four core and seven instrumental principles. 

For an AI system to be responsible, it should strive to maximize the four core principles. First, it should be designed to provide Beneficence, by working towards doing good for humanity. It should ensure users and stakeholders maintain their Autonomy as far as possible, and that the decisions made represent Justice, in that they are not biased or discriminating. Finally, the AI systems should be Non-maleficent, in that it should do as much as possible to prevent harm, whether that is towards humans, other living things, the environment or the planet as a whole.

To achieve this, AI systems should be designed based on a set of instrumental principles. These provide responsibility through supporting and facilitating for the core principles, and should be adapted to the organisation developing and using the system, the context it is used in, and the goals and implementations of the system itself. An AI system could be Transparent, in that decisions can be explained and understood, which typically leads to increased Justice and Autonomy. The system could be Accountable, in that responsibility for its decisions are distributed among organisations and individuals creating and using the system, which facilitate the Justice and Non-maleficence of the system. The same principles can be strengthened by ensuring the Privacy of the users and stakeholders impacted by the system. Finally, systems can be Sustainable, in that their environmental footprint is minimized, or Trustworthy, by ensuring they work optimally and as expected, both contributing to increasing the core principles.

As shown in \autoref{sec:results-barriers}, a key barrier preventing a wide adoption of responsible AI is the gap between abstract ethical principles and the concrete work being done by AI developers in the industry. To aid this, this paper developed a framework for selecting principles relevant for a project, and converting these to concrete implementations that can be followed, tested and used during development of the system. This principle is presented in \autoref{sec:framework}.


\subsection{The EPNIS framework}
\label{sec:framework}
% Give a general explanation of the framework (project-level framework for converting abstract ethical principles into concrete implementations)
The EPNIS-framework is a project-level framework created to help AI developers, designers and managers assess a proposed project, discover relevant ethical principles for the project and convert the abstract principles into concrete implementations that can be used during development. The framework is based on the Design for Values-approach described by \textcite{Dignum_2019}, but places it in a more concrete and development-focused framework. An overview of the EPNIS framework is given in \autoref{fig:PNIS-framework}.

\begin{figure}[htp]
    \centering
    \import{./Images/}{EPNIS-framework}
    \caption{Overview of the EPNIS framework}
    \label{fig:PNIS-framework}
\end{figure}

As calls have been made among the reviewed papers to include stakeholders in the development of responsible AI, these are included in all relevant steps of the framework. While selection of stakeholders depend on the project, it is crucial that this include as wide range of people as possible, in order to ensure everyone affected by the AI system gets to share their opinion on how the project should be developed.

\subsubsection{Steps of the EPNIS framework}
% Discuss each step in detail, including substeps
The EPNIS-framework consists of four main steps -- Existence, Principles, Norms, Implementations and Sustain (EPNIS), as well as one substep -- Prioritized principles. These steps are explained in detail in the following pages.

\paragraph{Existence}
Every responsible AI project should start with assessing whether the system can be developed responsibly and should exist in the first place. This entails checking whether the goals of the system are beneficial and non-maleficent, and that these can be achieved without negatively impacting autonomy and justice in any significant way. If this is not possible, then the system cannot be developed in a responsible way, and should be abandoned.

\paragraph{Principles}
Once it is clear that the project can be developed responsibly, the process moves to the second step of the framework -- Principles. Here, the development team should use relevant laws and regulations for their system and context, together with existing business principles for their organisation and stakeholder values, to select ethical principles that apply to the project. It is important to include a wide range of stakeholders in this step, to ensure that all possibly relevant principles are considered for the system.

As discussed in \autoref{sec:results-rq1}, core principles should be part of any responsible AI project, as they are the ones that directly contribute to the ethical value of a project. As such, the goal of this stage is to find the instrumental principles that contribute the most to the core principles of Autonomy, Beneficence, Non-maleficence and Justice.

\paragraph{Prioritized principles}
Once the project has a set of principles, the development team should combine internal prioritization of principles (from organisational strategies, organisation values, ethics panels and internal to the team) with stakeholders' prioritization, to create a list of prioritized principles. While ethical dilemmas may still appear, driving ethical discussion with no right or wrong answer, this gives the development team a starting point for the next steps, while also operating as a prioritization of limited resources.

\paragraph{Norms}
After prioritizing principles, the development team should utilize expert panels, stakeholders and the team itself to concretize the principles into norms, in the same way it should be done in the Design for Values-approach used by \textcite{Dignum_2019}. These norms are normative interpretations of the principles, and primarily state how the principles should be interpreted.

An example of converting a principle to a norm is shown by \textcite{Benjamins_2019}, who describes three different interpretations of the principle of fairness (included in the core principle of Justice): Independence, Separation and Sufficiency. To better show the differences, each interpretation will be illustrated with an AI system meant to approve loans, where gender is a sensitive variable. Positive predictions means that the system believes the applicant will pay back the full loan, and a true positive means it is correct in its beliefs. 

\begin{description}
    \item[Independence] means that the proportion of positive predictions are the same for all sensitive groups \parencite{Benjamins_2019}. In the example, this means that if half of all men are approved for loans, half of all women should also be approved for loans.
    
    \item[Separation] means that the true positive and false positive rate is the same in all sensitive groups \parencite{Benjamins_2019}. In the example, this means that if 80\% of all men that end up paying their loans are approved by the system, 80\% of all women that end up paying their loans should also be approved. At the same time, if 90\% of men who fail to pay their loans are not approved, then 90\% of women in the same situation should also not be approved.
    
    \item[Sufficiency] means that the Positive Predictive Value (i.e, the proportion of true positive predictions divided by all positive predictions) is the same for all sensitive groups \parencite{Benjamins_2019}. In the example, this means that if 95\% of men who are approved for loans end up paying them back, then 95\% of women who are approved for loans should end up paying them back.
\end{description}
As it is impossible to achieve all three, the development team for this hypothetical scenario would have to decide, using all available resources, which interpretation suits their context, organisation, system and personal beliefs the best.

\paragraph{Implementations}
When the norms of the project are in place, the development team should use relevant laws and regulations, implementations used in academia and by experts, as well as their own ideas to create concrete implementations of the selected norms. These implementations should ideally be testable and measurable, and should, following Design for Values \parencite{Dignum_2019}, be explicitly connected to a specific part of the system.

By creating concrete and testable implementations, these can be included in common software development practices, such as sprint plannings and code reviews \parencite{Dybå_2008_agile}. Each implementation is connected to a certain part of a system, making auditing of the implementation easy, as only specific parts of the system needs to be checked. As there is a direct line from principles, through norms to implementations, such audits also check that the norms, and by extension the principles, still apply to the system.


\paragraph{Sustain}
After all implementations have been designed, development of the AI system can begin. It is important that the developers keep the previously selected implementations in mind as they work on the system, to avoid accidentally disabling an implementation, thus disconnecting a norm from the system. This is made easier by connecting each implementation to specific parts of the system, as developers working on one part only need to remember the implementations connected to their area of work.

Modern software development is a flexible process \parencite{Dybå_2008_agile}, and ethical values and bias may change over time \parencite{Nauck_2019}. This makes it important to review the previous steps of the framework, and managers of AI development projects should ensure regular reviews are conducted.

Implementations are most closely related to the actual development process, and will shift as the project is developed. These should therefore be reviewed most frequently, to ensure that they are still implemented and relevant as the AI system adapts throughout its development. For AI organisations utilizing agile development methods, these reviews could be conducted as part of code- or sprint reviews \parencite{Dybå_2008_agile}.

Norms only shift when the interpretation of principles shift. As such shifts may occur due to changes in the context or organisation, changes in the goals or design of the system, or through larger societal changes, these should still be reviewed regularly, but less frequently than the implementations. While the actual frequency depends on the specifics of the context, organisation and system, an example is to review these on a biannual or quarterly basis. If any norms are changed or adjusted during these reviews, implementations should be reviewed again, to ensure that the connection between principles, norms and implementations still hold.

Principles are the least likely to shift, as this only occurs if laws and regulations, organisational values or societal values change. As such, these reviews may be conducted on a reactive basis, such as when relevant laws are changed, or on an annual, semi-annual or less frequent basis. If any principles, or the prioritization of these, are changed during the review, norms and implementations should also be reviewed again, to ensure that the connection between principles, norms and implementations still hold.

During the process of implementing and following the EPNIS framework, any decisions taken along the way and the results of each step, as well as the reviews, should be clearly documented and stored. To provide transparency and auditability to the system, these documents should be made available for audits when needed, and should thus be understandable by both internal and external auditing teams.

\subsubsection{The EPNIS framework compared to existing frameworks}
Some of the reviewed papers introduce frameworks for developing responsible AI. While the EPNIS framework has been designed to solve a previously unsolved gap between principles and practice, it overlaps with these existing frameworks on some accounts, and a comparison is therefore in order.

\textcite{Chen_2020} introduce a framework for AI governance, built around a national registry of AI systems. Their framework is focused on regulating AI systems, and therefore applies to AI organisations and government systems, rather than project-level implementations of principles. The EPNIS framework can be used as a safeguard for high-impact systems, can provide documentation that can be used for creating annual reports, and can be used as a step towards increased auditability, all things that are included as part of the framework introduced by \textcite{Chen_2020}.

\textcite{Dignum_2019} provides the Design for Values framework. As this is used as foundation for the EPNIS framework, the two align with each other, most notably through the connection between principles, norms and implementations. Although the frameworks are somewhat alike, the EPNIS framework focuses on the implementation details, including relevant inputs that should be used for each step, whereas Design for Values primarily attempts to connect existing software development methodologies to responsible AI. Instead of directly comparing the two frameworks, then, they should be used alongside each other, using Design for Values to find principles, norms and implementations, and using EPNIS to systematize the process, find the right inputs for each step and ensure steps are reviewed when needed.

AI4People, the framework introduced by \textcite{Floridi_2018}, is mostly focused on how policy makers may facilitate for responsible AI. The two frameworks therefore do not compete with each other. Instead, EPNIS can be used to aid some of the action points proposed in AI4People, such as by facilitating for auditing through documentation of each step.

\textcite{Peters_2020} present two frameworks for responsible AI -- the Responsible Design Process (RDP), and the Spheres of Technology. The Responsible Design Process consists of five steps showcasing different development stages, with Research, Insights, Ideation, Prototypes and Evaluation. Although the framework was originally developed to responsibly design new technology, it has great potential for working alongside the EPNIS framework. The first two stages, Research and Insight, are used to discover needs and preferences from stakeholders. These can be used to either support, or fully conduct, step two and three of EPNIS -- Principles and Norms -- where the goal is to combine different inputs, including from stakeholders, to create principles and norms for the system. The next two steps of the Responsible Design Process, Ideation and Prototypes, take previously generated insights and create solutions based on them, and can therefore be used to generate implementations from the previously discovered norms. Finally, the Responsible Design Process's Evaluation and EPNIS's Sustain have the same goal, continually reassessing the responsibility of a system. This shows that the two frameworks -- the Responsible Design Process and EPNIS -- have great potential for success when used together.

The second framework introduced by \textcite{Peters_2020}, the Spheres of Technology, maps the impact of technology to six different spheres -- Adoption, Interface, Task, Behavior, Life, and Society. This is differs from EPNIS in multiple ways, but can be used to ensure the selected principles, norms and implementations cover all possible impacts of the AI system.

\textcite{Werder_2022} introduce a framework for mitigating bias, thus supporting Justice, through data governance. This may be used as an implementation for responsible AI systems developed using the EPNIS framework, if it is relevant to the context, organisation and design of the system being developed.

The framework introduced by \textcite{Wright_2018} is designed to gather expectations and inputs from stakeholders, and evaluating how well these expectations are met. This could integrate with the EPNIS framework in two ways. First, the framework from \textcite{Wright_2018} could be used during the Existence-step of the EPNIS framework, to evaluate whether the stakeholders' expectations can realistically be met in a responsible way, and thus whether the system should exist in the first place. Secondly, the expectations can be used to create principles and norms when following the EPNIS framework, as they give a good indication of stakeholder values, prioritizations and concretizations. The two frameworks, then, should be used alongside each other to develop responsible AI systems.

Some additional papers also introduce what they refer to as frameworks for responsible AI. These are, however, targeting laws and regulations \parencite{Brand_2022} or the finance industry \parencite{Rizinski_2022}, or focused on defining responsible AI's place in society \parencite{Buhmann_2021}, categorizing algorithms \parencite{Cheng_2021}, assessing the impact of AI \parencite{Havrda_2020}, mapping expectations of socio-technical systems \parencite{Minkkinen_2021} or evaluating how organisational culture impacts responsible AI initiatives \parencite{Rakova_2021}, rather than finding ways to implement responsible AI. As such, comparing these to the EPNIS framework is not relevant for this paper.


\subsection{Observations}
% Lack of quantitative research
% Lack of stakeholder inclusion in the research
As shown in \autoref{tab:summary-methodology}, only five of the reviewed papers (9\%) conducted quantitative surveys, and only four (7\%) conducted qualitative surveys. Of these, only five papers included surveys conducted on stakeholders not working directly on developing AI (i.e., representative samples of the public, healthcare professionals, etc.). This shows that most of the reviewed papers look inward, at previous research, existing guidelines, experts, and professionals developing AI systems. While basing work on existing research is important \parencite{Tranfield_2003}, this prevents new inputs from outsiders, and could potentially lead to interesting insights from stakeholders being missed. Similar exclusion of stakeholders has also been found in commercial guidelines for responsible AI, where only 38\% of the papers reviewed by \textcite{Bélisle-Pipon_2022} engaged stakeholders -- and only 9\% involved ordinary citizens. A worst-case scenario is that stakeholders are left out of the process of defining what constitutes a responsible and ethical AI system. As the goal of responsible AI is to develop AI systems that operate for the good of humanity, this could ultimately lead to a collapse of the whole research field.


The geographical distribution, as shown in \autoref{tab:summary-context}, show a bias towards Western research, and thus the potential for a bias in the reviewed principles. This geographical bias is reflected in the distribution of guideline documents that are available. \textcite{AlgorithmWatch} contains a set of 167 guidelines for developing responsible AI. While this is not necessarily an exhaustive list, the database has been in use since 2019 and is open for submissions \parencite{AlgorithmWatch_about}, making it a representative source. Of the 167 guidelines in the database, only 14 (8\%) are from Asian countries, 1 (0.6\%) from Southern Africa and no guidelines come from South America or Northern Africa. As mentioned in \autoref{sec:results-principles-instrumental}, ethical values may change depending on the geographical background of relevant stakeholders. As such, this bias may limit the generalizability of the current state of responsible AI, especially for AI systems in Asian, South American or African contexts.


% Discuss differences between responsible AI principles and RRI principles (Dignum 2019, p. 58)
% Relate back to ethics and RRI
Both responsible AI and RRI (as discussed in \autoref{sec:background}) have the same goal -- developing responsible solutions to societal problems. The responsible AI principles discussed in this paper are quite different from the conceptual dimensions used by RRI, as the AI principles primarily focus on the outcomes of the system, while RRI primarily focus on the innovation process. Still, the methodology them employ are still the same -- both strategies involve stakeholders and base their development process on a set of principles, with the goal of achieving responsibility. These same observations have also been noted among the reviewed papers. \textcite[p.~346]{Hacker_2022} mention that responsible AI is \textquote{similar, but still quite disconnected} from RRI. Similarly, \textcite{Dignum_2019} argue that RRI can be applied to responsible AI development. Based on this, AI developers and managers should adopt methods and tools from RRI, with the goal of achieving responsibility in both the resulting system, as well as the research and innovation processes leading up to the finished system.


% "Ethics washing" / Ethics shopping (Morley, barriers)
In a time when countries are accused of using sports for sportswashing \parencite{Fruh_2022_sportswashing,Wearing_2022_sportswashing}, organisations are accused of using fake eco-friendliness for greenwashing \parencite{Gibbens_2022_greenwashing}, and even the US military is accused of using Pride for pinkwashing \parencite{Kane_2022_pinkwashing}, some (e.g. \cite{Gianni_2022,Havrda_2020,Morley_2021}) have raised concerns that responsible AI can be used for ethics washing, i.e., that organisations may publicly adopt ethical principles to show that their AI systems are responsible and ethical, without changing the underlying, unethical practice \parencite{Morley_2021}. These organisations would then claim benefits of responsible AI, such as increased stakeholder trust and long-term profitability, without actually having responsible systems. To avoid this, organisations should set long-term responsible goals, and publicly show their processes towards responsibility, in addition to the overarching principles they have adopted \parencite{Rakova_2021}. By being transparent about how responsible AI is achieved, not just the abstract principles constituting the goal, organisations can ensure their stakeholders that the AI systems they interact with are actually safe, thus sparing them from such accusations.


% Trust is core outcome, but rarely used as principle
An interesting observation is that trust is mentioned as one of the core advantages of developing responsible AI, yet it is one of the least popular principles among the reviewed papers. Although pinpointing the exact reason for this is hard, a potential reason is that trust requires two parties, a trustor, i.e., the party that trusts, and a trustee, i.e., the party that is trusted \parencite{Huang_2006_trust}. In comparison, the other principles, such as beneficence, only require one party, the party that creates benefits. This means that trust is not something that can be created internally in an organisation or AI system, but must be created in partnership with trustors. By using stakeholders as trustors, it is clear that trust is something that must be created through having trustworthy systems, and should therefore be considered as the output of responsible AI, rather than the input.


\subsection{Limitations}
\label{sec:Limitations}
In the words of \textcite[p.~2160]{Morley_2020}, \textquote{all research projects have their limitations and this one is no exception.} Although this paper attempts to create an understanding of how AI systems can be responsible, it does so by solely looking at academic papers. As shown by \textcite{AlgorithmWatch}, much work has been done on developing guidelines for responsible AI in both governments and industries around the world. Although some of the reviewed papers have reviewed these (e.g., \cite{Bélisle-Pipon_2022,Fjeld_2020,Jobin_2019,Ryan_2021}) excluding non-academic guidelines from the review brings with it the potential for missing several important observations, principles and frameworks that are not included in the reviewed papers.

% Potential bias even with selected methodology
Although the methodology of systematic literature reviews is designed to minimize bias, \textcite{Tranfield_2003} highlights the eligibility assessment of papers to be a potential source for bias, as subjective opinion is needed to evaluate whether a given document should be included in the review. This is especially true for this paper, as all the research has been conducted by one person. This leaves limited possibility to discuss any uncertainties, although my supervisor has available for some cases. Although this creates a risk that the selection of papers -- and thus the included principles -- may be biased, the fact that the found principles are typically used in multiple reviewed papers indicates that the results of this paper are likely to be relatively generalizable.


AI ethics is a wide field. A key limitation of this research is that it solely focused on one type of ethical AI -- responsible AI. Several of the reviewed papers mention more of these types, and \autoref{tab:synonyms} shows some of these, together with the number of articles using the term on Scopus. Interestingly, several of the included types have more results than responsible AI. This shows that while including multiple types would increase the scope of the paper, doing so would also provide new perspectives on how to develop AI responsibly, with the potential of uncovering new principles, norms and implementations, as well as frameworks and tools to convert these to practice.

\begin{table}[ht]
    \centering
    \caption[Search results for types of ethical AI]{Search results for types of ethical AI. Searches were conducted on December 14, 2022, using the search string \texttt{"synonym AI" OR "synonym Artificial Intelligence"}, for example, \texttt{"Responsible AI" OR "Responsible Artificial Intelligence"}. Searches looked at article titles, abstracts and keywords.}
    \label{tab:synonyms}
    \begin{tabular}{p{0.4\textwidth}c}
    \toprule
        \textbf{Synonym} & \textbf{Scopus results} \\
    \midrule
        Explainable AI & 4260 \\
        AI ethics & 620 \\
        Trustworthy AI & 356 \\
        Ethical AI & 267 \\
        Human-centred AI & 245 \\
        Responsible AI & 237 \\
        AI for social good & 74 \\
        Sustainable AI & 48 \\
        Fair AI & 42 \\
    \bottomrule
    \end{tabular}
\end{table}


\subsection{Future research}
% A complete taxonomy of the field of AI ethics (all alternative terms for responsible AI)
Ethical AI included several, somewhat overlapping terms, as shown in \autoref{tab:synonyms}. To unify this wide range of terms, future research should consider creating a complete taxonomy for the field of responsible AI. As taxonomies \textquote{help humans classify objects according to similarities and differences} \parencite{Kundisch_2022} and formally create a unifying definition of terms \parencite{Uschold_1996}, this could create a set of agreed-upon terms, with clearly defined separations, that would allow research to move in the same direction. Although this has not yet been done for the field of AI ethics, significant efforts have gone into creating taxonomies of explainable AI (e.g., \cite{BarredoArrieta_2020,Clinciu_2019,Graziani_2022,Bellucci_2021,Brennen_2020}), and these could be used as a starting point for mapping the whole field of AI ethics.


\autoref{sec:definition-ai} looks at different definitions of artificial intelligence used throughout the reviewed papers. While a deep-dive into definitions and terminology is outside of the scope of this paper, there appears to be a lack of consensus regarding how to define the concept of artificial intelligence, which should be considered the backbone of responsible AI. As such, AI researchers should work together with researchers within the field of philosophy and linguistics to create a unifying, agreed-upon definition of artificial intelligence. This, much like the above-mentioned taxonomy, would align future research along agreed-upon lines, thus ensuring work is moving the field in the same direction, while limiting disagreements and confusion.


As shown in \autoref{fig:principle-counts}, the most used principles among the reviewed papers are Transparency and Justice. \textcite[p.~103]{Hagendorff_2020} claim that this is because these principles reflect \textquote{the \textquote{male way} of thinking about ethical problems}, in that they are focused on, and solvable by, technical tools and methods. To increase the adoption of the other principles discussed in \autoref{sec:results-rq1}, especially the remaining core principles of Autonomy, Beneficence and Non-maleficence, future research should work on developing tools and methods for facilitating, measuring and evaluating these. Two benefits would arise from being able to measure and evaluate the remaining principles. First, this would give AI developers and managers a better view of the responsibility of their systems, and would provide them with tools to ensure a targeted level of responsibility is achieved. Secondly, this would give lawmakers and governments an avenue for creating new laws that can ensure the responsibility of AI systems. As laws and regulations are major antecedents for responsible AI development, this would likely lead to an increase in the responsibility of both existing and new AI systems.


% Only barely touches the field of AI governance, but that is well done by (Gianni R., Lehtinen S., Nieminen M.)
While the goal of this paper is to understand how to develop responsible AI systems, little attention has been paid to AI governance, or organisational governance as a whole. While the introduced framework is designed to push AI development in a responsible direction, this only makes up a small part of a much wider field of AI governance \parencite{Papagiannidis_2022_governance}. \textcite{Gianni_2022} discuss some methods for AI governance, but note that existing methods are limited. Future research should look at ways to properly govern AI systems in order to ensure they are responsible, and should consider how the EPNIS frameworks fit with existing, and future, methods for doing so.

% Work is needed to empirically evaluate the framework
Although the EPNIS framework is proposed to bridge the gap between abstract principles and actual development, it remains to be tested in real scenarios. Work should therefore be done to empirically evaluate the proposed framework, with the goal of both testing that it helps bridge the gap, as well as to discover ways the framework should be expanded to better fulfill this goal. This work can be done by conducting case studies, thus actually develop AI systems using the framework; by qualitative studies, such as surveying AI developers and -professionals to evaluate whether the framework could be applicable, useful and relevant for their work; or by quantitative studies, such as surveying AI organisations to estimate the probability of the framework being able to help bridge the gap for a given organisation.


Although several studies call for the inclusion of stakeholders in responsible AI development, few of the reviewed papers survey stakeholders. Future research should therefore be conducted with a focus on stakeholders of AI systems. This would could take several forms. Some work should attempt to map stakeholders for a given AI system, thus making it easier for developers to discover relevant stakeholders for their work. These kind of stakeholder maps have been created in several other domains, such as fossil fuel \parencite{Yudha_2018_stakeholdermap}, digital health \parencite{An_2022_stakeholdermap} and nanotechnology \parencite{Hansen_2010_stakeholdermap}, so this work only needs to apply existing methods to the field of responsible AI. Other work could aim directly at the general public, to learn what principles and norms are important to laypersons. This work should first employ qualitative methods to get an overview of all potentially relevant principles and norms, before using quantitative methods to gain an understanding of which principles and norms are important to the general public. Although \textcite{Jakesch_2022} has already conducted one such survey, their work is based solely in the US, so more work is needed to be able to generalize the results.

The EPNIS framework argues that organisations looking to develop responsible AI systems should find relevant principles, norms and implementations for their context, organisation and system to be developed. While the framework mentions inputs that should be considered, the actual process of finding these principles, norms and implementations is not described in the framework, nor in the rest of the paper. Although a selection of principles are described in \autoref{sec:results-rq1}, highlighting that every AI system should focus on core principles, no methodology is provided for evaluating potential instrumental principles for a given system. As the selection process will vary depending on the context of a system, future research should apply the framework to different contexts and systems, with the goal of finding similarities in both processes and results that can be generalized. These findings should be used to both expand the current knowledge on how to assess principles for responsible AI, as well as to extend the EPNIS framework with new knowledge and tools.


As there have been complaints regarding the barrier between academic research and responsible AI work in the industry, future research should adopt an implementation-based view, and prioritize ways to concretize and implement their findings in real-life scenarios.


% Dropped:
% - Connections between the field of ethics (i.e., non-AI ethics, ethical theory and philosophy) and responsible AI?
% - % Consider ways to democratically create frameworks / guidelines. Create guidelines anchored in the general public in a democratic way (Gianni R., Lehtinen S., Nieminen M.)
%   - Create guidelines / principles rooted in public participation
% - To handle ethics washing: Evaluate whether organisational practices and AI systems actually changes after implementing guidelines, or if they are a PR-tool. I.e., check if guidelines actually contribute to change, have an impact