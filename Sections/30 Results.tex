\section{Findings}
\label{sec:Results}
The systematic search was conducted on October 4, 2022. An overview of the literature review, including each step of the process, is shown in \autoref{fig:PRISMA-flowchart}.

\todo[inline]{Move PRISMA to end of method, rather than results.}
% Number of papers per stage, results from the methodology
\begin{figure}[p]
    \centering
    \import{./Images/}{PRISMA-flowchart}
    \caption[Overview of literature review]{Overview of literature review. Adapted from \textcite{PRISMA_2022}.}
    \label{fig:PRISMA-flowchart}
\end{figure}


\subsection{Descriptive Results} % See Lukkien et al.
As shown in \autoref{fig:PRISMA-flowchart}, a total of 289 unique titles were assessed during the literature review. After filtering on titles, abstracts, and the full text, and after adding sources from snowballing, 54 documents were included in the review. An overview of the reviewed papers can be seen in \autoref{tab:paper-overview} (\autoref{app:overview}).

\autoref{tab:summary-year} shows the yearly distribution of the assessed papers, for each step of the literature review. The oldest of the assessed papers was published in 2015, with the majority of publications happening in 2019-2022 ($n = 279$; 97\%). The same distribution is reflected in the reviewed papers, where the oldest paper was published in 2017, and 94\% of the papers ($n = 50$) were published between 2019-2022. This indicates a young field of science, that has just recently gained traction. Although the field is still young, discussions with my supervisor led to the conclusion that 289 unique results in the preliminary search means the field has been explored enough to conduct a systematic literature review.

\begin{table}[!ht]
    \centering
    \caption{Yearly distribution of the assessed papers.}
    \label{tab:summary-year}
    \begin{threeparttable}
    \begin{tabular}{lcccc}
    \toprule
        \multirow{2}{*}{\textbf{Year}}
            & \multirow{2}{*}{\textbf{Raw data}}
            & \multirow{2}{*}{\shortstack{\textbf{After title}\\\textbf{filtering}}}
            & \multirow{2}{*}{\shortstack{\textbf{After abstract}\\\textbf{filtering}}}
            & \multirow{2}{*}{\textbf{Review}} \\ \\
    \midrule
        \textbf{2015} & 1 & 0 & 0 & 0 \\ 
        \textbf{2016} & 1 & 0 & 0 & 0 \\ 
        \textbf{2017} & 0 & 0 & 0 & 1 \\ 
        \textbf{2018} & 2 & 2 & 1 & 3 \\ 
        \textbf{2019} & 16 & 14 & 3 & 8 \\ 
        \textbf{2020} & 45 & 34 & 9 & 10 \\ 
        \textbf{2021} & 113 & 80 & 24 & 17 \\ 
        \textbf{2022} & 105 & 83 & 20 & 15 \\ 
        \textbf{2023} & 1 & 1 & 0 & 0 \\ 
        \textbf{No year}\tnote{*} & 5 & 3 & 0 & 0 \\ 
    \midrule
        \textbf{Total} & \textbf{289} & \textbf{217} & \textbf{57} & \textbf{54} \\ 
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] No year provided in the databases.
    \end{tablenotes}
\end{threeparttable}
\end{table}

% Compare primary vs secondary studies
While the field is relatively young, the methodology used by researchers within the field, as shown in \autoref{tab:summary-methodology}, indicates a certain level of maturity. \textcite[p. 930]{Keathley-Herring_2016} states that \textquote{a research area should evolve from an exploratory beginning to conceptual frameworks being proposed and tested, and then to industry exposure and finally, a convergence on best practices and consistent terminology.} The papers included in the review employ a wide range of methodologies, of both an exploratory (reviews, expert discussions), conceptual (conceptual frameworks, viewpoints) and evaluative (qualitative interviews, quantitative surveys, case studies) nature. Some authors (e.g., \cite{Fjeld_2020,Jobin_2019}) have noted a convergence within the proposed principles as well, indicating that the field is approaching an agreed-upon set of best practices.

The fact that the research being done shows a high level of maturity while the actual field is young may be due to the connections between responsible AI and more established fields within AI- and software ethics. These connections are further explored in \autoref{sec:results-alernative-terms}.

\begin{table}[ht]
    \centering
    \caption{Methodologies used in the reviewed papers.}
    \label{tab:summary-methodology}
    \begin{threeparttable}
    \begin{tabular}{p{0.4\linewidth}cc}
    \toprule
        \textbf{Methodology} & \textbf{Count} & \textbf{Percentage}\tnote{*} \\
    \midrule
        Review of guidelines    & 9     & 17  \\
        Literature review       & 9     & 17  \\
        Qualitative interviews  & 9     & 17  \\
        Conceptual              & 7     & 13  \\
        Unknown                 & 6     & 11  \\
        Quantitative survey     & 5     & 9   \\
        Viewpoint               & 4     & 7   \\
        Case study              & 4     & 7   \\
        Expert discussion       & 3     & 6   \\
        Review of AI strategies & 2     & 4   \\
        Review of laws          & 1     & 2   \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] Percentage of reviewed papers that use the methodology. Note that some papers use more than one methodology, so the percentages do not add up to 100.
    \end{tablenotes}
\end{threeparttable}
\end{table}

\autoref{tab:summary-context} shows the geographical context of the reviewed papers. Over half the papers ($n = 32$; 59\%) are created without a given context. This is relatively natural, as conceptual papers, viewpoints and literature reviews are rarely limited to geographical locations. All the reviewed papers are written in English, and the included literature reviews are also predominantly reviews of English papers. This creates a possibility for a Western bias, even in papers that appear to be written without a specific context.

For the rest of the papers, most are focused on a Western context ($n = 11$; 21\%), with some work being done in China ($n = 2$; 4\%), India ($n = 2$; 4\%) and South Africa ($n = 1$; 2\%). This geographical bias is reflected in the distribution of guideline documents that are available.
\todo{Possibly move this to discussion?}
\textcite{AlgorithmWatch} contains a set of 167 guidelines for developing responsible AI. While this is by no means an exhaustive list, the database has been in use since 2019 and is open for submissions \parencite{AlgorithmWatch_about}, making it a reliable source. Of the 167 guidelines in the database, only 14 (8\%) are from Asian countries, 1 (0.6\%) from Southern Africa and no guidelines come from South America or Northern Africa.

\begin{table}[htpb]
    \centering
    \caption{Geographical context of the reviewed papers.}
    \label{tab:summary-context}
    \begin{threeparttable}
    \begin{tabular}{p{0.4\linewidth}cc}
    \toprule
        \textbf{Location} & \textbf{Count} & \textbf{Percentage}\tnote{*} \\
    \midrule
        Europe                          & 8     & 15 \\ 
        Global                          & 6     & 11 \\ 
        The US                          & 2     & 4 \\
        China                           & 2     & 4 \\ 
        India                           & 2     & 4 \\ 
        Western countries in general    & 1     & 2 \\ 
        South Africa                    & 1     & 2 \\
        N/A\tnote{\textdagger}          & 32    & 59 \\ 
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] Percentage of reviewed papers that focuses on the given location.
        \item [\textdagger] No context given. Includes viewpoints, conceptual papers, etc.
    \end{tablenotes}
\end{threeparttable}
\end{table}

As with geographical context, most of the reviewed papers ($n = 36$; 67\%) are written without a specific domain in mind. This is, once again, natural for conceptual papers, viewpoints and literature reviews, of which there are many among the reviewed papers. The rest of the papers are primarily focused on healthcare ($n = 7$; 13\%). Healthcare is a domain with a wide use of technological solutions (e.g. \cite{Azaria_2016,Martinez_2008,Son_2014}), including AI-based solutions (e.g. \cite{Singh_2023,Kumar_2023}). As health records are digitized \parencite{Rajkomar_2018}, yet more avenues for adopting AI opens up, with estimates valuing the data of NHS patients in the UK at around 10 billion GBP per year \parencite{Downey_2019}. At the same time, healthcare uses large amounts of personal data, thus increasing the sensitivity patients feel when interacting with systems, automatic or not \parencite{Gupta_2021}. Professionals in the field of healthcare are therefore used to ethical principles and guidelines, such as the Hippocratic Oath \parencite{Wiesing_2020}, and calls have been made to use medical ethics as a starting point for responsible AI guidelines \parencite{DaltonBrown_2020, Siala_2022}.

Outside of healthcare, papers tend to look at laws and governance ($n = 3$; 6\%) and AI developers ($n = 3$; 6\%). Both these cases are natural focus areas for responsible AI. As will be discussed in \autoref{sec:results-rq2:antecedents}, the primary antecedent for responsible AI is existing laws and regulations, with national AI strategies also working to push AI systems towards responsibility. Therefore, focusing on how laws and governance impacts -- and should impact -- the field of responsible AI is important, in order to ensure laws and strategies provide the best possible protection of laypersons, while limiting innovation as little as possible. In a similar fashion, the connection between responsible AI and AI developers is natural. Developers are those who actually design, create and maintain AI systems in use, and are therefore a natural source for understanding current practices in the industry, as well as which principles can be adopted, and how they should be adopted, in order to have the largest impact on moving AI in a responsible direction. A complete overview of the domains used in the reviewed papers can be seen in \autoref{tab:summary-domain}.

\begin{table}[htpb]
    \centering
    \caption{Domains of the reviewed papers.}
    \label{tab:summary-domain}
    \begin{threeparttable}
    \begin{tabular}{p{0.4\linewidth}cc}
    \toprule
        \textbf{Domain} & \textbf{Count} & \textbf{Percentage}\tnote{*} \\
    \midrule
        Healthcare              & 7     & 13 \\ 
        Laws and governance     & 3     & 6 \\
        Developers              & 3     & 6 \\
        Labor                   & 1     & 2 \\
        Education               & 1     & 2 \\
        Finance                 & 1     & 2 \\
        Water                   & 1     & 2 \\
        Multi-sector            & 1     & 2 \\
        N/A\tnote{\textdagger}  & 36    & 67 \\ 
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] Percentage of reviewed papers that focuses on the given domain.
        \item [\textdagger] No domain given. Includes viewpoints, conceptual papers, etc.
    \end{tablenotes}
\end{threeparttable}
\end{table}

The reviewed papers come from a wide variety of sources, including journals from several different fields, books, and proceedings. An easy way to quickly assess the quality of a paper is by evaluating the rank of its journal or publisher \parencite{Keathley-Herring_2016}. In Norway, this is primarily done by the Norwegian Register for Scientific Journals, Series and Publishers, who uses a 3-level ranking system, with level 2 being the highest level, and 0 meaning that a journal was rejected from the register \parencite{kanalregisteret}. Using this register, \autoref{tab:summary-ranks} shows the ranks of the reviewed journals. No 0-ranked journals means that the papers can be considered good enough quality to use for a review.

\begin{table}[htpb]
    \centering
    \caption{Journal ranks of the reviewed papers.}
    \label{tab:summary-ranks}
    \begin{threeparttable}
    \begin{tabular}{p{0.4\linewidth}cc}
    \toprule
        \textbf{Journal rank} & \textbf{Count} & \textbf{Percentage}\tnote{*} \\
    \midrule
        2                       & 6     & 11    \\
        1                       & 34    & 63    \\
        0                       & 0     & 0     \\
        N/A\tnote{\textdagger}  & 14    & 26    \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] Percentage of reviewed papers from a journal with the given rank.
        \item [\textdagger] Journal rank not available. Includes books, proceeding papers and journals that have not been evaluated by the Norwegian Register.
    \end{tablenotes}
\end{threeparttable}
\end{table}

% Mention non-peer-reviewed papers included in the review, and why they were included
Of the 54 papers used for this review, two are from sources that are not peer reviewed -- \textcite{Fjeld_2020} and \textcite{Floridi_2019}. Both these sources were gathered during snowballing, and are included for the same reason. \textcite{Fjeld_2020} compares 36 AI principles, and is considered \textquote{a landmark in the synthesis of AI ethics principles and guidance} \parencite[p. 2]{Bélisle-Pipon_2022}. Likewise, \textcite{Floridi_2019} reviewed six sets of principles, and supports the conclusions drawn by \textcite{Floridi_2018}. As the papers are central for the field, and referenced in multiple of the other papers, they were deemed relevant enough to be included in this review.

% Cite studies that might appear to meet the inclusion criteria, but which were excluded, and explain why they were excluded.
Some papers may appear to meet the inclusion criteria, but were still included. Examples of this include \textcite{ElHaddadeh_2021} and \textcite{Trocin_2021}. \textcite{ElHaddadeh_2021} presents an in-depth comparison of AI used for healthcare in the UK (NHS Test and Trace) and Quatar (EHTERAZ), looking at methods used for achieving responsibility in the two systems, and how well the systems performed at stopping the spread of COVID-19. Although the paper looks at ways AI can be used ethically, it neither defines responsible AI, contains or critizises a framework nor discusses antecedents or business advantages of responsible AI, and therefore does not answer the eligibility criteria. \textcite{Trocin_2021} performs a literature review aimed at ethical issues in healthcare, and how AI may help solve or mitigate these issues. While the topic of the article revolves around using AI ethically, its focus is primarily on ethical issues of healthcare, rather than ethical issues of AI, and the paper therefore does not answer the eligibility criteria. The rest of the papers rejected during full-text assessment (step 5, \autoref{sec:method-steps}) were rejected for similar reasons.


\subsection{Key concepts}
\todo[inline]{Remove own definitions, instead adopt from one of the reviewed papers (and defend it, argue why I am using that one)}
\todo[inline]{Remove block quotes (except for the quotes I am using in this text, where I \textit{can} keep them)}
\import{Sections/}{35 Definitions}


\subsection{RQ1 -- Principles for responsible AI}
\label{sec:results-rq1}
Of the 54 reviewed papers, 48 (89\%) present or use a set of principles to create or describe responsible AI, for a total of 58 unique principles. Three steps are used to make this set of principles more manageable. First, principles were clustered following the categorization done by \textcite{Ryan_2021}. Their work reviewed existing guidelines, and came up with 11 categories, containing 61 subcategories, which most of the reviewed principles aligned with. The principles that did not align with any subcategory were categorized with the closest fit, with principles that did not align with any clusters being grouped as \textit{Others}. This ensured all principles would be used in the review, without creating multiple low-density clusters.

After clustering the principles, some changes were made to the categories proposed by \textcite{Ryan_2021}. Clusters with fewer than five mentions among the reviewed texts were removed, and their principles moved to other clusters. This meant that \textit{Dignity} ($n = 3$) was moved to be part of \textit{Non-maleficence}, and \textit{Solidarity} ($n = 1$) became part of \textit{Justice and fairness}. As dignity can be considered to be a harm, and thus can be avoided by ensuring AI systems can do no harm, and as solidarity can be created by ensuring systems act in a fair and just manner, this change felt like a natural decision. Then, some categories were renamed -- \textit{Justice and fairness}, and \textit{Freedom and autonomy} were simplified to \textit{Justice} and \textit{Autonomy}, respectively. This was done to better aligned with common usage throughout the reviewed papers (e.g., \cite{Balagué_2021,Floridi_2018}, and the papers basing their principles on these). 

% Discuss split of principles (core and non-core)
After clusters had been created, these were again divided into two sets -- \emph{core} and \emph{instrumental} principles. This division was argued by \textcite{Canca_2020}, in order to separate core principles, that are \textquote{intrinsically valuable} (p. 19), from instrumental principles, \textquote{whose values are derived from their instrumental effect in protecting and promoting intrinsic values} (p. 20). The motivation for this was to align principles of AI ethics with a \textquote{widely utilized set of core principles in applied ethics} \parencite[p. 19]{Canca_2020}.

One key change was done to the division suggested by \textcite{Canca_2020}. \textit{Beneficence}, as suggested by \textcite{Canca_2020} to encapsulate both \textquote{avoiding
harm} and \textquote{doing good} (p. 19), was split into \textit{Beneficence} (doing good) and \textit{Non-maleficence} (avoiding harm). This has three benefits. First, it brings the division in line with the categories proposed by \textcite{Ryan_2021}, which are already used for the clustering mentioned above. Secondly, this aligns core principles of responsible AI with those used in other fields of applied ethics, such as bioethics \parencite{Beauchamp_2001} and medicine \parencite{Gillon_2003,Gillon_1994}. Finally, this resonates with principles used by several of the reviewed papers (e.g., \cite{Balagué_2021,Floridi_2018,Jobin_2019,Nauck_2019}), ensuring principles that are actually used within the field of responsible AI are placed in the group they belong.


\subsubsection{Core principles}
Core principles are described by \textcite{Canca_2020} as principles that \textquote{invoke those values that theories in moral and political philosophy argue to be intrinsically valuable, meaning their value is not derived from something else} (p. 19). She argues that core principles, then, should be present in every responsible AI system, and that they -- by themselves -- should be used to select the correct action for a given context, as the action that fulfills the core principles to the highest degree \parencite{Canca_2020}.

As mentioned above, this paper adapts the core principles suggested by \textcite{Canca_2020} to be the following clusters of principles: \textit{Autonomy}, \textit{Beneficence}, \textit{Non-maleficence} and \textit{Justice}. A complete overview of the core principles used by the reviewed papers is shown in \autoref{tab:paper-principles-core} (\autoref{app:principles}).

The notion that core principles should be present in every responsible AI system is somewhat reflected in the reviewed papers. Of the 48 papers that include a set of principles, 18 of them (38\%) include all four core principles. In comparison, only 6 (13\%) contain none of the core principles. Perhaps most importantly, most sets of principles created by reviewing existing guidelines for responsible AI contain all four (\cite{Clarke_2019,Floridi_2018,Jobin_2019,Ryan_2021}) or three (\cite{Fjeld_2020}) core principles. The only exceptions from this are \textcite{Brand_2022} and \textcite{Hagendorff_2020}, who only include one core principle each. \textcite{Brand_2022} is focused on developing a framework for responsible AI policy and regulation, and this may be explained by him primarily targeting more easily measureable principles, such as transparency and privacy. \textcite{Hagendorff_2020}, however, presents an interesting case. Although he only includes one of the core principles (\textit{Fairness}, which is included in \textit{Justice}) in his list of recurring principles, his overview \parencite[Table 1]{Hagendorff_2020} shows that both \textquote{common good} (\textit{Beneficence}), \textquote{human oversight} (\textit{Autonomy}) and \textquote{safety, cybersecurity} (\textit{Non-maleficence}) are commonly used among the guidelines he reviews. This indicates that core principles of responsible AI are extensively used in existing guidelines in the AI industry. 

\paragraph{Autonomy}
Autonomy can be defined as \textquote{the quality or state of being self-governing} and \textquote{self-directing freedom and especially moral independence} \parencite{dictionary_autonomy}. In relation to responsible AI, autonomy can be thought of as \textquote{the idea that individuals have a right to make decisions for themselves about the treatment they do or not receive} \parencite[p. 697-698]{Floridi_2018}. Following the categorization done by \textcite[Table 1]{Ryan_2021}, \textit{Autonomy} includes the sub-categories Freedom, Autonomy, Consent, Choice, Self-determination, Liberty, and Empowerment. Autonomy is included in 24 papers (50\% of all principle-including papers).

\textcite[p. 5]{Rothenberger_2019} argue that \textquote{An AI should
have a purpose,} and that this purpose should be to support, and not replace, humans. \textcite[p. 387]{Thelisson_2018}, \textcite[p. 102]{Lu_2022}. \textcite[p. 53]{Fjeld_2020} and \textcite[p. 416]{Clarke_2019} all stress that AI should stay under human control. Among the guidelines reviewed by \textcite[p. 11]{Jobin_2019}, autonomy is referred to as both a \textquote{positive freedom,} i.e., a freedom to do good things, which includes \textquote{the freedom to flourish, to self-determination through democratic means, the right to establish and develop relationships with other human beings, the freedom to withdraw consent, or the freedom to use a preferred platform or technology,} as well as a \textquote{negative freedom}, i.e., a freedom from bad things, such as \textquote{freedom from technological experimentation, manipulation or surveillance.} According to the guidelines reviewed by \textcite[p. 698]{Floridi_2018}, a key part of ensuring autonomy of humans is by limiting the autonomy of machines, ensuring that humans \textquote{retain the power to decide which decisions to take.} 

The reviewed papers
\todo{Should this be moved to discussion?}
contain a wide set of methods to ensure human autonomy in responsible AI systems. First, responsible AI systems should be built to ensure the freedoms listed by \textcite{Jobin_2019} are not impacted. This means responsible AI systems should not be used for manipulation or surveillance, nor should it limit the freedoms of humans. Secondly, responsible AI systems should ensure people affected by their decisions have a way to \textquote{challenge the use or output of the system} \parencite[p. 102]{Lu_2022}, \textquote{request and receive human review of those decisions} \parencite[p. 53]{Fjeld_2020} and \textquote{to opt out of automated decision} \parencite[p. 54]{Fjeld_2020}. AI developers should ensure informed consent is given before data is collected from people \parencite{Lukkien_2021,Jobin_2019}, and implement methods for humans to take control and override the automatic system when necessary \parencite{Floridi_2018,Rakova_2021}. \textcite{Liu_2021} argue that AI systems should adapt to each user, ensuring each person gets the level of autonomy and assistance as they need. 

Some of the reviewed papers (e.g., \cite{Brand_2022,Nevanperä_2021}) argue that responsible AI systems should have a \textquote{human in the loop.} This entails having a human involved in and responsible for the decisions made by the AI system, and comes with two benefits. First, a clear agency for the system is created, as the person is responsible even if decisions are automated \parencite{Nevanperä_2021}. Secondly, and perhaps more important, this lets a human \textquote{perform checks and/or investigate situations with considerable uncertainty} \parencite[p. 12]{vanBruxvoort_2021}. Manually handling unclear decisions means that the AI stays under human control, and limits autonomy no more than a fully manual system would.


\paragraph{Beneficence}
Beneficence is defined as \textquote{the quality or state of doing or producing good} \parencite{dictionary_beneficence}. This means that AI systems fulfilling the principle of beneficence should work towards the greater good, i.e., solutions that are good for humanity. According to \textcite{Ryan_2021}, \textit{Beneficence} include the sub-categories Benefits, Beneficence, Well-being, Peace, Social good, and Common good. Beneficence is included in 27 (56\%) of the papers.

According to \textcite[p. 9]{vanBruxvoort_2021}, AI systems fulfilling the principle of \textit{Beneficence} should have the \textquote{well-being of humans, society and planet put upfront.} Beneficent AI systems are systems created to be beneficial for humanity \parencite{Floridi_2018}, promoting good \parencite{Jobin_2019} and increase societal well-being \parencite{Mikalef_2022}. \textcite[p. 103]{BarredoArrieta_2020} argue that responsible AI systems should \textquote{be aligned with the United Nation’s Sustainable Development Goals and contribute to them in a positive and tangible way.} In a similar fashion, \textcite[p. 2]{Buhmann_2021} highlight responsible AI's \textquote{responsibility to do good,} i.e., it should work towards \textquote{improvement of living conditions, such as in accordance with the sustainable development goals.} Responsible AI should \textquote{complement humans} \parencite[p. 419]{Clarke_2019}, and \textquote{be developed and used to increase prosperity for all,} thus advancing \textquote{inclusive growth and sustainable development} \parencite[p. 97534]{Rizinski_2022}.

Several methods exist to ensure AI systems are beneficial. On an organizational level, \textcite[p. 2151]{Morley_2020} argue that AI systems should be justified, i.e., \textquote{the purpose for building the system must be clear and linked to a clear benefit.} To support this, organisations \textquote{should define as clear as possible what it views as beneficial to humans [\ldots], so that its view can guide decisions for all aspects} \parencite[p. 9]{vanBruxvoort_2021}. In other words, AI organisations should create an organisational definition of beneficence, which can be used in their AI development in the same way corporate visions and values can be used. Beneficence should be considered when deciding what problems to solve, and beneficial AI systems should work towards \textquote{solutions to some of the world’s greatest problems, such as curing diseases, ensuring food security and preventing environmental damage} \parencite[p. 73]{Ryan_2021}. Responsible AI organisations should work towards \textquote{minimizing power concentration,} \textquote{working more closely with ‘affected’ people,} and \textquote{minimizing conflicts of interests} \parencite[p. 11]{Jobin_2019}.

Beneficial AI systems should include stakeholders in their decision making \parencite{Lu_2022,Havrda_2020,Morley_2020}, ensuring that everyone who are impacted by the system have a say. \textcite[p. 134]{Nevanperä_2021} argues that, due to the wide reach of AI systems, \textquote{all sentient beings ought to be considered as stakeholders.} In order to properly ensure systems are beneficial, they should include metrics that can be used as key performance indicators to evaluate the beneficence of the system \parencite{Eitel-Porter_2021}. These may, for example, be \textquote{metrics related to satisfaction with life, affect, psychological well-being, and inequality} \parencite[p. 5]{Havrda_2020}.


\paragraph{Non-maleficence}
Maleficence is defined as \textquote{the act of committing harm or evil,} leading non-maleficence to mean the avoidance of committing harm. \textcite[p. 697]{Floridi_2018} define non-maleficence simply as \textquote{do no harm.} Following \textcite{Ryan_2021}, \textit{Non-maleficence} includes the sub-categories Non-maleficence, Security, Safety, Harm, Protection, Precaution, Prevention, Integrity, and Non-subversion. Non-maleficence is the second most used core principle amongst the reviewed papers, with 32 papers (67\%) including the principle.

\textcite[p. 6]{Doorn_2021} define non-maleficence as \textquote{an intention to avoid needless harm or injury,} and states that \textquote{it is often interpreted as the need to protect safety and security.} People creating and using non-maleficent AI systems should assess, control for and mitigate potentially harmful impacts \parencite[p. 2]{Buhmann_2021}, in both the short- and long term \parencite[p. 416]{Clarke_2019}. Non-maleficent systems should have \textquote{resistance to malfunctions (robustness) and recoverability when malfunctions occur (resilience)} \parencite[p. 416]{Clarke_2019}, and \textquote{ensure data security and AI safety} \parencite[p. 2]{Buhmann_2021}. \textcite[p. 13]{Anagnostou_2022} argue that non-maleficent AI systems should focus \textquote{not only security but also cybersecurity.} % Only got to Doorn, I am going to bed


Much like with beneficence, multiple papers argue that stakeholder involvement is an important step towards achieving non-maleficent AI \parencite[p. 416]{Clarke_2019}


\paragraph{Justice}


\subsubsection{Instrumental principles}
Whereas core principles are intrinsically valuable, instrumental principles provide value by supporting and facilitating for the core principles \parencite{Canca_2020}. Which instrumental principles to include when developing a responsible AI system is therefore up to the developers and designers of the system. As ethics is not absolute, the selection of instrumental principles may vary depending on, amongst others, the cultural context of the AI system \parencite{vanBruxvoort_2021}, the support system of the country the AI system is to be used in \parencite{Wright_2018}, or the ethnicity, gender, political leaning and background of afflicted stakeholders \parencite{Jakesch_2022}, and may even change over time \parencite{vanBruxvoort_2021}.

Based on the clustering described above, the reviewed papers include the following categories of instrumental principles: \textit{Transparency}, \textit{Accountability}, \textit{Trust}, \textit{Sustainability}, \textit{Privacy} and \textit{Others}, where the latter encapsulates Data governance, Laws and regulations, Informed consent, Predictability, Quality benchmarking, Interventions and co-design, and System performance. A complete overview of the instrumental principles used by the reviewed papers is shown in \autoref{tab:paper-principles-instrumental} (\autoref{app:principles}).

While this list encompasses instrumental principles used by the reviewed papers, it
\todo{Maybe move this paragraph to discussion?} 
is by no means an exhaustive list of potential principles designers of AI systems can use, nor is that the intention of this section. Instead, this section is meant to give an overview of some principles that are used by current literature on responsible AI, in order to show some methods to facilitate for the core principles -- the principles that actually make an AI system responsible -- mentioned above. While the principles mentioned here are relatively common, and can thus work as a starting point for selecting relevant instrumental principles for a new system, designers should ensure they are adapted to the system and context they operate in, and that relevant principles that are not mentioned here are also included in their projects.

\paragraph{Transparency}
% Both system-, process- and technical
% Some (Floridi) include this with core
% Jobin: Not principle per se, but rather an enabling condition

\paragraph{Accountability}

\paragraph{Trust}

\paragraph{Sustainability}

\paragraph{Privacy}

\paragraph{Others}
% Data governance
% Laws and regulations
% Informed consent
% Predictability
% Quality benchmarking
% Interventions and co-design
% System performance



% Include how the principles build upon each other / connections between principles
%   - I.e., how explainability is needed for the other principles to be applied
% Include also ways to enable the different principles, i.e. column "How to enable principles"

\subsubsection{How to enable responsible AI}
% Possibly include some generic "How to enable principles" advice that apply to multiple principles
%   - E.g. data provenance (Werder K., Ramesh B., Zhang R.S.)

\subsection{RQ2 -- Antecedents for responsible AI}
\label{sec:results-rq2:antecedents}

\subsection{RQ3 -- Business advantages of responsible AI}
\label{sec:results-rq3:advantages}
% Academic stuff

% Outside of academia... (see Snowballing sources)


\subsection{Alternative terms for responsible AI}
\label{sec:results-alernative-terms}
% A comparison of the different terms for responsible AI I have seen throughout the review, and how they compare to each other.
% Use as base for future research -- a complete taxonomy of the field, establishing common and standardized terms
% Possibly a comparison of SCOPUS hits for the given terms, to show their popularity in comparison to each other

% \subsection{Challenges with responsible AI}
\subsection{Criticisms of responsible AI}
\label{sec:results-criticism}
% What challenges / barriers have the review shown?
% I.e., lack of implementation guides / most guidelines are very general and vague (maybe a "how to implement RAI" subsection?)
%   - Are also found in general ethics: https://www.scopus.com/record/display.uri?eid=2-s2.0-0025411067&origin=resultslist&sort=cp-f&src=s&st1=autonomy+AND+beneficence+AND+justice&nlo=&nlr=&nls=&sid=4734d518162d88ad8f175bc2a82e4e4f&sot=b&sdt=b&sl=41&s=ALL%28autonomy+AND+beneficence+AND+justice%29&relpos=9&citeCnt=324&searchTerm=
% "Ethics washing"
% Lack of quantitative research