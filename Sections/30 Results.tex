\section{Findings}
\label{sec:Results}
The findings of this section are structured as follows. First, descriptive results are given, describing metadata of the reviewed articles. Then, \autoref{sec:Definitions} contains definitions of key concepts, derived from the reviewed papers. Finally, RQ1, RQ2 and RQ3 are answered in \autoref{sec:results-rq1}, \ref{sec:results-rq2:antecedents} and \ref{sec:results-rq3:advantages}, respectively.

\subsection{Descriptive Results} % See Lukkien et al.
\label{sec:results-descriptive}
As shown in \autoref{fig:PRISMA-flowchart}, a total of 289 unique titles were assessed during the literature review. After filtering on titles, abstracts, and the full text, and after adding sources from snowballing, 54 documents were included in the review. An overview of the reviewed papers can be seen in \autoref{tab:paper-overview} (\autoref{app:overview}).

\autoref{tab:summary-year} shows the yearly distribution of the assessed papers, for each step of the literature review. The oldest of the assessed papers was published in 2015, with the majority of publications happening in 2019-2022 ($n = 279$; 97\%). The same distribution is reflected in the reviewed papers, where the oldest paper was published in 2017, and 94\% of the papers ($n = 50$) were published between 2019-2022. This indicates a young field of science, that has just recently gained traction. Although the field is still young, discussions with my supervisor led to the conclusion that 289 unique results in the preliminary search means the field has been explored enough to conduct a systematic literature review.

\begin{table}[!ht]
    \centering
    \caption{Yearly distribution of the assessed papers.}
    \label{tab:summary-year}
    \begin{threeparttable}
    \begin{tabular}{lcccc}
    \toprule
        \multirow{2}{*}{\textbf{Year}}
            & \multirow{2}{*}{\textbf{Raw data}}
            & \multirow{2}{*}{\shortstack{\textbf{After title}\\\textbf{filtering}}}
            & \multirow{2}{*}{\shortstack{\textbf{After abstract}\\\textbf{filtering}}}
            & \multirow{2}{*}{\textbf{Review}} \\ \\
    \midrule
        \textbf{2015} & 1 & 0 & 0 & 0 \\ 
        \textbf{2016} & 1 & 0 & 0 & 0 \\ 
        \textbf{2017} & 0 & 0 & 0 & 1 \\ 
        \textbf{2018} & 2 & 2 & 1 & 3 \\ 
        \textbf{2019} & 16 & 14 & 3 & 8 \\ 
        \textbf{2020} & 45 & 34 & 9 & 10 \\ 
        \textbf{2021} & 113 & 80 & 24 & 17 \\ 
        \textbf{2022} & 105 & 83 & 20 & 15 \\ 
        \textbf{2023} & 1 & 1 & 0 & 0 \\ 
        \textbf{No year}\tnote{*} & 5 & 3 & 0 & 0 \\ 
    \midrule
        \textbf{Total} & \textbf{289} & \textbf{217} & \textbf{57} & \textbf{54} \\ 
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] No year provided in the databases.
    \end{tablenotes}
\end{threeparttable}
\end{table}

% Compare primary vs secondary studies
While the field is relatively young, the methodology used by researchers within the field, as shown in \autoref{tab:summary-methodology}, indicates a certain level of maturity. \textcite[p.~930]{Keathley-Herring_2016} states that \textquote{a research area should evolve from an exploratory beginning to conceptual frameworks being proposed and tested, and then to industry exposure and finally, a convergence on best practices and consistent terminology.} The papers included in the review employ a wide range of methodologies, of both an exploratory (reviews, expert discussions), conceptual (conceptual frameworks, viewpoints) and evaluative (qualitative interviews, quantitative surveys, case studies) nature. Some authors (e.g., \cite{Fjeld_2020,Jobin_2019}) have noted a convergence within the proposed principles as well, indicating that the field is approaching an agreed-upon set of best practices.

The fact that the research being done shows a high level of maturity while the actual field is young may be due to the connections between responsible AI and more established fields within AI- and software ethics. This is further discussed in \autoref{sec:Discussion}.

\begin{table}[ht]
    \centering
    \caption{Methodologies used in the reviewed papers.}
    \label{tab:summary-methodology}
    \begin{threeparttable}
    \begin{tabular}{p{0.4\linewidth}cc}
    \toprule
        \textbf{Methodology} & \textbf{Count} & \textbf{Percentage}\tnote{*} \\
    \midrule
        Review of guidelines    & 9     & 17  \\
        Literature review       & 9     & 17  \\
        Qualitative interviews  & 9     & 17  \\
        Conceptual              & 7     & 13  \\
        Unknown                 & 6     & 11  \\
        Quantitative survey     & 5     & 9   \\
        Viewpoint               & 4     & 7   \\
        Case study              & 4     & 7   \\
        Expert discussion       & 3     & 6   \\
        Review of AI strategies & 2     & 4   \\
        Review of laws          & 1     & 2   \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] Percentage of reviewed papers that use the methodology. Note that some papers use more than one methodology, so the percentages do not add up to 100.
    \end{tablenotes}
\end{threeparttable}
\end{table}

\autoref{tab:summary-context} shows the geographical context of the reviewed papers. Over half the papers ($n = 32$; 59\%) are created without a given context. This is relatively natural, as conceptual papers, viewpoints and literature reviews are rarely limited to geographical locations. All the reviewed papers are written in English, and the included literature reviews are also predominantly reviews of English papers. This creates a possibility for a Western bias, even in papers that appear to be written without a specific context. For the rest of the papers, most are focused on a Western context ($n = 11$; 21\%), with some work being done in China ($n = 2$; 4\%), India ($n = 2$; 4\%) and South Africa ($n = 1$; 2\%). 

\begin{table}[htpb]
    \centering
    \caption{Geographical context of the reviewed papers.}
    \label{tab:summary-context}
    \begin{threeparttable}
    \begin{tabular}{p{0.4\linewidth}cc}
    \toprule
        \textbf{Location} & \textbf{Count} & \textbf{Percentage}\tnote{*} \\
    \midrule
        Europe                          & 8     & 15 \\ 
        Global                          & 6     & 11 \\ 
        The US                          & 2     & 4 \\
        China                           & 2     & 4 \\ 
        India                           & 2     & 4 \\ 
        Western countries in general    & 1     & 2 \\ 
        South Africa                    & 1     & 2 \\
        N/A\tnote{\textdagger}          & 32    & 59 \\ 
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] Percentage of reviewed papers that focuses on the given location.
        \item [\textdagger] No context given. Includes viewpoints, conceptual papers, etc.
    \end{tablenotes}
\end{threeparttable}
\end{table}

As with geographical context, most of the reviewed papers ($n = 36$; 67\%) are written without a specific domain in mind. This is, once again, natural for conceptual papers, viewpoints and literature reviews, of which there are many among the reviewed papers. The rest of the papers are primarily focused on healthcare ($n = 7$; 13\%). Healthcare is a domain with a wide use of technological solutions (e.g. \cite{Azaria_2016,Martinez_2008,Son_2014}), including AI-based solutions (e.g. \cite{Singh_2023,Kumar_2023}). As health records are digitized \parencite{Rajkomar_2018}, yet more avenues for adopting AI opens up, with estimates valuing the data of NHS patients in the UK at around 10 billion GBP per year \parencite{Downey_2019}. At the same time, healthcare uses large amounts of personal data, thus increasing the sensitivity patients feel when interacting with systems, automatic or not \parencite{Gupta_2021}. Professionals in the field of healthcare are therefore used to ethical principles and guidelines, such as the Hippocratic Oath \parencite{Wiesing_2020}, and calls have been made to use medical ethics as a starting point for responsible AI guidelines \parencite{DaltonBrown_2020, Siala_2022}.

Outside of healthcare, papers tend to look at laws and governance ($n = 3$; 6\%) and AI developers ($n = 3$; 6\%). Both these cases are natural focus areas for responsible AI. As will be discussed in \autoref{sec:results-rq2:antecedents}, the primary antecedent for responsible AI is existing laws and regulations, with national AI strategies also working to push AI systems towards responsibility. Therefore, focusing on how laws and governance impacts -- and should impact -- the field of responsible AI is important, in order to ensure laws and strategies provide the best possible protection of laypersons, while limiting innovation as little as possible. In a similar fashion, the connection between responsible AI and AI developers is natural. Developers are those who actually design, create and maintain AI systems in use, and are therefore a natural source for understanding current practices in the industry, as well as which principles can be adopted, and how they should be adopted, in order to have the largest impact on moving AI in a responsible direction. A complete overview of the domains used in the reviewed papers can be seen in \autoref{tab:summary-domain}.

\begin{table}[htpb]
    \centering
    \caption{Domains of the reviewed papers.}
    \label{tab:summary-domain}
    \begin{threeparttable}
    \begin{tabular}{p{0.4\linewidth}cc}
    \toprule
        \textbf{Domain} & \textbf{Count} & \textbf{Percentage}\tnote{*} \\
    \midrule
        Healthcare              & 7     & 13 \\ 
        Laws and governance     & 3     & 6 \\
        Developers              & 3     & 6 \\
        Labor                   & 1     & 2 \\
        Education               & 1     & 2 \\
        Finance                 & 1     & 2 \\
        Water                   & 1     & 2 \\
        Multi-sector            & 1     & 2 \\
        N/A\tnote{\textdagger}  & 36    & 67 \\ 
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] Percentage of reviewed papers that focuses on the given domain.
        \item [\textdagger] No domain given. Includes viewpoints, conceptual papers, etc.
    \end{tablenotes}
\end{threeparttable}
\end{table}

The reviewed papers come from a wide variety of sources, including journals from several different fields, books, and proceedings. An easy way to quickly assess the quality of a paper is by evaluating the rank of its journal or publisher \parencite{Keathley-Herring_2016}. In Norway, this is primarily done by the Norwegian Register for Scientific Journals, Series and Publishers, who uses a 3-level ranking system, with level 2 being the highest level, and 0 meaning that a journal was rejected from the register \parencite{kanalregisteret}. Using this register, \autoref{tab:summary-ranks} shows the ranks of the reviewed journals. No 0-ranked journals means that the papers can be considered good enough quality to use for a review.

\begin{table}[htpb]
    \centering
    \caption{Journal ranks of the reviewed papers.}
    \label{tab:summary-ranks}
    \begin{threeparttable}
    \begin{tabular}{p{0.4\linewidth}cc}
    \toprule
        \textbf{Journal rank} & \textbf{Count} & \textbf{Percentage}\tnote{*} \\
    \midrule
        2                       & 6     & 11    \\
        1                       & 34    & 63    \\
        0                       & 0     & 0     \\
        N/A\tnote{\textdagger}  & 14    & 26    \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item [*] Percentage of reviewed papers from a journal with the given rank.
        \item [\textdagger] Journal rank not available. Includes books, proceeding papers and journals that have not been evaluated by the Norwegian Register.
    \end{tablenotes}
\end{threeparttable}
\end{table}

% Mention non-peer-reviewed papers included in the review, and why they were included
Of the 54 papers used for this review, two are from sources that are not peer reviewed -- \textcite{Fjeld_2020} and \textcite{Floridi_2019}. Both these sources were gathered during snowballing, and are included for the same reason. \textcite{Fjeld_2020} compares 36 AI principles, and is considered \textquote{a landmark in the synthesis of AI ethics principles and guidance} \parencite[p.~2]{Bélisle-Pipon_2022}. Likewise, \textcite{Floridi_2019} reviewed six sets of principles, and supports the conclusions drawn by \textcite{Floridi_2018}. As the papers are central for the field, and referenced in multiple of the other papers, they were deemed relevant enough to be included in this review.

% Cite studies that might appear to meet the inclusion criteria, but which were excluded, and explain why they were excluded.
Some papers may appear to meet the inclusion criteria, but were still included. Examples of this include \textcite{ElHaddadeh_2021} and \textcite{Trocin_2021}. \textcite{ElHaddadeh_2021} presents an in-depth comparison of AI used for healthcare in the UK (NHS Test and Trace) and Quatar (EHTERAZ), looking at methods used for achieving responsibility in the two systems, and how well the systems performed at stopping the spread of COVID-19. Although the paper looks at ways AI can be used ethically, it neither defines responsible AI, contains or critizises a framework nor discusses antecedents or business advantages of responsible AI, and therefore does not answer the eligibility criteria. \textcite{Trocin_2021} performs a literature review aimed at ethical issues in healthcare, and how AI may help solve or mitigate these issues. While the topic of the article revolves around using AI ethically, its focus is primarily on ethical issues of healthcare, rather than ethical issues of AI, and the paper therefore does not answer the eligibility criteria. The rest of the papers rejected during full-text assessment (step 4, \autoref{fig:PRISMA-flowchart}) were rejected for similar reasons.


\subsection{Key concepts}
\import{Sections/}{35 Definitions}


\subsection{RQ1 -- Principles for responsible AI}
\label{sec:results-rq1}
Of the 54 reviewed papers, 48 (89\%) present or use a set of principles to create or describe responsible AI, for a total of 56 unique principles. Three steps are used to make this set of principles more manageable. First, principles were clustered following the categorization done by \textcite{Ryan_2021}. Their work reviewed existing guidelines, and came up with 11 categories -- Transparency, Justice and fairness, Non-maleficence, Responsibility, Privacy, Beneficence, Freedom and autonomy, Trust, Sustainability, Dignity, and Solidarity -- containing a total of 61 subcategories, which most of the reviewed principles aligned with. The principles that did not align with any subcategory were categorized with the closest fit, with principles that did not align with any clusters being grouped as \textit{Others}. This ensured all principles would be used in the review, without creating multiple low-density clusters.

It is important to note that there may be some overlap between the principles. An example of this can be seen in \textcite{Fjeld_2020}, where the principle of \textit{Accountability} includes as Environmental Responsibility as a subcategory, or in \textcite{Mikalef_2022}, where the principle of \textit{Societal and environmental well-being} is included in both \textit{Beneficence} and \textit{Sustainability}. In these cases, principles are either included in both clusters, which is the case with \textcite{Mikalef_2022}, or in the one that aligns the most, as done with \textcite{Fjeld_2020}, depending on how much of the principle overlaps.

After clustering the principles, some changes were made to the categories proposed by \textcite{Ryan_2021}. Clusters with fewer than five mentions among the reviewed texts were removed, and their principles moved to other clusters. This meant that \textit{Dignity} ($n = 3$) was moved to be part of \textit{Non-maleficence}, and \textit{Solidarity} ($n = 1$) became part of \textit{Justice and fairness}. As dignity can be considered to be a harm, and thus can be avoided by ensuring AI systems can do no harm, and as solidarity can be created by ensuring systems act in a fair and just manner, this change felt like a natural decision. Then, some categories were renamed -- \textit{Justice and fairness}, and \textit{Freedom and autonomy} were simplified to \textit{Justice} and \textit{Autonomy}, respectively. This was done to better aligned with common usage throughout the reviewed papers (e.g., \cite{Balagué_2021,Floridi_2018}, and the papers basing their principles on these).

% Discuss split of principles (core and non-core)
After clusters had been created, these were again divided into two sets -- \emph{core} and \emph{instrumental} principles. This division was argued by \textcite{Canca_2020}, in order to separate core principles, that are \textquote{intrinsically valuable} (p.~19), from instrumental principles, \textquote{whose values are derived from their instrumental effect in protecting and promoting intrinsic values} (p.~20). The motivation for this was to align principles of AI ethics with a \textquote{widely utilized set of core principles in applied ethics} \parencite[p.~19]{Canca_2020}.

One key change was done to the division suggested by \textcite{Canca_2020}. \textit{Beneficence}, as suggested by \textcite{Canca_2020} to encapsulate both \textquote{avoiding
harm} and \textquote{doing good} (p.~19), was split into \textit{Beneficence} (doing good) and \textit{Non-maleficence} (avoiding harm). This has three benefits. First, it brings the division in line with the categories proposed by \textcite{Ryan_2021}, which are already used for the clustering mentioned above. Secondly, this aligns core principles of responsible AI with those used in other fields of applied ethics, such as bioethics \parencite{Beauchamp_2001} and medicine \parencite{Gillon_2003,Gillon_1994}. Finally, this resonates with principles used by several of the reviewed papers (e.g., \cite{Balagué_2021,Floridi_2018,Jobin_2019,Nauck_2019}), ensuring principles that are actually used within the field of responsible AI are placed in the group they belong.

The distribution of principles used in the reviewed papers is shown in \autoref{fig:principle-counts}.

\begin{figure}[ht]
    \centering
    \import{Images}{principle-bar}
    \caption{Distribution of principles in the reviewed papers}
    \label{fig:principle-counts}
\end{figure}


\subsubsection{Core principles}
Core principles are described by \textcite{Canca_2020} as principles that \textquote{invoke those values that theories in moral and political philosophy argue to be intrinsically valuable, meaning their value is not derived from something else} (p.~19). She argues that core principles, then, should be present in every responsible AI system, and that they -- by themselves -- should be used to select the correct action for a given context, as the action that fulfills the core principles to the highest degree \parencite{Canca_2020}.

As mentioned above, this paper adapts the core principles suggested by \textcite{Canca_2020} to be the following clusters of principles: \textit{Autonomy}, \textit{Beneficence}, \textit{Non-maleficence} and \textit{Justice}. A complete overview of the core principles used by the reviewed papers is shown in \autoref{tab:paper-principles-core} (\autoref{app:principles}).

The notion that core principles should be present in every responsible AI system is somewhat reflected in the reviewed papers. Of the 48 papers that include a set of principles, 18 of them (38\%) include all four core principles. In comparison, only 6 (13\%) contain none of the core principles. Perhaps most importantly, most sets of principles created by reviewing existing guidelines for responsible AI contain all four (\cite{Clarke_2019,Floridi_2018,Jobin_2019,Ryan_2021}) or three (\cite{Fjeld_2020}) core principles. The only exceptions from this are \textcite{Brand_2022} and \textcite{Hagendorff_2020}, who only include one core principle each. \textcite{Brand_2022} is focused on developing a framework for responsible AI policy and regulation, and this may be explained by him primarily targeting more easily measureable principles, such as transparency and privacy. \textcite{Hagendorff_2020}, however, presents an interesting case. Although he only includes one of the core principles (\textit{Fairness}, which is included in \textit{Justice}) in his list of recurring principles, his overview \parencite[Table 1]{Hagendorff_2020} shows that both \textquote{common good} (\textit{Beneficence}), \textquote{human oversight} (\textit{Autonomy}) and \textquote{safety, cybersecurity} (\textit{Non-maleficence}) are commonly used among the guidelines he reviews. This indicates that core principles of responsible AI are extensively used in existing guidelines in the AI industry. 

\paragraph{Autonomy}
Autonomy can be defined as \textquote{the quality or state of being self-governing} and \textquote{self-directing freedom and especially moral independence} \parencite{dictionary_autonomy}. In relation to responsible AI, autonomy can be thought of as \textquote{the idea that individuals have a right to make decisions for themselves about the treatment they do or not receive} \parencite[p.~697-698]{Floridi_2018}. Following the categorization done by \textcite[Table 1]{Ryan_2021}, \textit{Autonomy} includes the subcategories Freedom, Autonomy, Consent, Choice, Self-determination, Liberty, and Empowerment. Autonomy is included in 24 papers (50\% of all principle-including papers).

\textcite[p.~5]{Rothenberger_2019} argue that \textquote{An AI should
have a purpose,} and that this purpose should be to support, and not replace, humans. \textcite[p.~387]{Thelisson_2018}, \textcite[p.~102]{Lu_2022}. \textcite[p.~53]{Fjeld_2020} and \textcite[p.~416]{Clarke_2019} all stress that AI should stay under human control. Among the guidelines reviewed by \textcite[p.~11]{Jobin_2019}, autonomy is referred to as both a \textquote{positive freedom,} i.e., a freedom to do good things, which includes \textquote{the freedom to flourish, to self-determination through democratic means, the right to establish and develop relationships with other human beings, the freedom to withdraw consent, or the freedom to use a preferred platform or technology,} as well as a \textquote{negative freedom}, i.e., a freedom from bad things, such as \textquote{freedom from technological experimentation, manipulation or surveillance.} According to the guidelines reviewed by \textcite[p.~698]{Floridi_2018}, a key part of ensuring autonomy of humans is by limiting the autonomy of machines, ensuring that humans \textquote{retain the power to decide which decisions to take.} 

The reviewed papers contain a wide set of methods to ensure human autonomy in responsible AI systems. First, responsible AI systems should be built to ensure the freedoms listed by \textcite{Jobin_2019} are not impacted. This means responsible AI systems should not be used for manipulation or surveillance, nor should it limit the freedoms of humans. Secondly, responsible AI systems should ensure people affected by their decisions have a way to \textquote{challenge the use or output of the system} \parencite[p.~102]{Lu_2022}, \textquote{request and receive human review of those decisions} \parencite[p.~53]{Fjeld_2020} and \textquote{to opt out of automated decision} \parencite[p.~54]{Fjeld_2020}. AI developers should ensure informed consent is given before data is collected from people \parencite{Lukkien_2021,Jobin_2019}, and implement methods for humans to take control and override the automatic system when necessary \parencite{Floridi_2018,Rakova_2021}. \textcite{Liu_2021} argue that AI systems should adapt to each user, ensuring each person gets the level of autonomy and assistance as they need. 

Some of the reviewed papers (e.g., \cite{Brand_2022,Nevanperä_2021}) argue that responsible AI systems should have a \textquote{human in the loop.} This entails having a human involved in and responsible for the decisions made by the AI system, and comes with two benefits. First, a clear agency for the system is created, as the person is responsible even if decisions are automated \parencite{Nevanperä_2021}. Secondly, and perhaps more important, this lets a human \textquote{perform checks and/or investigate situations with considerable uncertainty} \parencite[p.~12]{vanBruxvoort_2021}. Manually handling unclear decisions means that the AI stays under human control, and limits autonomy no more than a fully manual system would.


\paragraph{Beneficence}
Beneficence is defined as \textquote{the quality or state of doing or producing good} \parencite{dictionary_beneficence}. This means that AI systems fulfilling the principle of beneficence should work towards the greater good, i.e., solutions that are good for humanity. According to \textcite{Ryan_2021}, \textit{Beneficence} include the subcategories Benefits, Beneficence, Well-being, Peace, Social good, and Common good. Beneficence is included in 27 (56\%) of the papers.

According to \textcite[p.~9]{vanBruxvoort_2021}, AI systems fulfilling the principle of \textit{Beneficence} should have the \textquote{well-being of humans, society and planet put upfront.} Beneficent AI systems are systems created to be beneficial for humanity \parencite{Floridi_2018}, promoting good \parencite{Jobin_2019} and increase societal well-being \parencite{Mikalef_2022}. \textcite[p.~103]{BarredoArrieta_2020} argue that responsible AI systems should \textquote{be aligned with the United Nation’s Sustainable Development Goals and contribute to them in a positive and tangible way.} In a similar fashion, \textcite[p.~2]{Buhmann_2021} highlight responsible AI's \textquote{responsibility to do good,} i.e., it should work towards \textquote{improvement of living conditions, such as in accordance with the sustainable development goals.} Responsible AI should \textquote{complement humans} \parencite[p.~419]{Clarke_2019}, and \textquote{be developed and used to increase prosperity for all,} thus advancing \textquote{inclusive growth and sustainable development} \parencite[p.~97534]{Rizinski_2022}.

Several methods exist to ensure AI systems are beneficial. On an organisational level, \textcite[p.~2151]{Morley_2020} argue that AI systems should be justified, i.e., \textquote{the purpose for building the system must be clear and linked to a clear benefit.} To support this, organisations \textquote{should define as clear as possible what it views as beneficial to humans [\dots], so that its view can guide decisions for all aspects} \parencite[p.~9]{vanBruxvoort_2021}. In other words, AI organisations should create an organisational definition of beneficence, which can be used in their AI development in the same way corporate visions and values can be used. Beneficence should be considered when deciding what problems to solve, and beneficial AI systems should work towards \textquote{solutions to some of the world’s greatest problems, such as curing diseases, ensuring food security and preventing environmental damage} \parencite[p.~73]{Ryan_2021}. Responsible AI organisations should work towards \textquote{minimizing power concentration,} \textquote{working more closely with ‘affected’ people,} and \textquote{minimizing conflicts of interests} \parencite[p.~11]{Jobin_2019}.

Beneficial AI systems should include stakeholders in their decision making \parencite{Lu_2022,Havrda_2020,Morley_2020}, ensuring that everyone who are impacted by the system have a say. \textcite[p.~134]{Nevanperä_2021} argue that, due to the wide reach of AI systems, \textquote{all sentient beings ought to be considered as stakeholders.} In order to properly ensure systems are beneficial, they should include metrics that can be used as key performance indicators to evaluate the beneficence of the system \parencite{Eitel-Porter_2021}. These may, for example, be \textquote{metrics related to satisfaction with life, affect, psychological well-being, and inequality} \parencite[p.~5]{Havrda_2020}.


\paragraph{Non-maleficence}
Maleficence is defined as \textquote{the act of committing harm or evil,} leading non-maleficence to mean the avoidance of committing harm. \textcite[p.~697]{Floridi_2018} define non-maleficence simply as \textquote{do no harm.} Following \textcite{Ryan_2021}, \textit{Non-maleficence} includes the subcategories Non-maleficence, Security, Safety, Harm, Protection, Precaution, Prevention, Integrity, and Non-subversion. Non-maleficence is the second most used core principle amongst the reviewed papers, with 32 papers (67\%) including the principle.

\textcite[p.~6]{Doorn_2021} define non-maleficence as \textquote{an intention to avoid needless harm or injury,} and states that \textquote{it is often interpreted as the need to protect safety and security.} People creating and using non-maleficent AI systems should assess, control for and avoid or mitigate potentially harmful impacts \parencites{Buhmann_2021,Fjeld_2020}, in both the short- and long term \parencite[p.~416]{Clarke_2019}, and of both an accidental and deliberate nature \parencite[p.~697]{Floridi_2018}. This includes both direct impacts, such as autonomous weapon systems being used for killing \parencite{Eitel-Porter_2021}, but also indirect impacts, such as job displacement \parencite[p.~12]{Gupta_2021} and power concentration \parencite{Jobin_2019}.

Non-maleficent AI systems should have \textquote{resistance to malfunctions (robustness) and recoverability when malfunctions occur (resilience)} \parencite[p.~416]{Clarke_2019}, resist both internal and external manipulations \parencite{Fjeld_2020,Rothenberger_2019}, \textquote{not diminish or destroy but respect, preserve or even increase human dignity} \parencite[p.~13]{Jobin_2019}, and \textquote{ensure data security and AI safety} \parencite[p.~2]{Buhmann_2021}, through means like data governance \parencite{Werder_2022}. \textcite[p.~13]{Anagnostou_2022} argue that developers of non-maleficent AI systems should focus \textquote{not only security but also cybersecurity,} i.e., protection from attacks \parencite[p.~70]{Ryan_2021}. 

Non-maleficent AI can be achieved by involving stakeholders \parencite{Jobin_2019,Dignum_2021}, using their perspectives in an impact assessment and including them in the design process \parencite[p.~416]{Clarke_2019}, and to help better identify potential impacts \parencite{Havrda_2020}. Such impact assessments should be documented and regularly reviewed \parencite[p.~70]{Ryan_2021}, and create a basis for risk management methods, designed to minimize the risk of harmful consequences \parencite{Buhmann_2021,Clarke_2019,Brand_2022}. AI systems should be regularly tested, audited and validated \parencite{Jobin_2019,Fjeld_2020,vanBruxvoort_2021}, for example by automatic checks of safety metrics developed for the system  \parencite{Havrda_2020}. AI organisations should ensure their data is protected, regularly updated and only available to workers who needs access \parencite{vanBruxvoort_2021,Mikalef_2022}. Finally, if the potential harm of a system is too large, organisations should \textquote{consider alternative, less harmful ways of achieving the same objectives} \parencite[p.~419]{Clarke_2019}, such as by using less sensitive data \parencite[p.~10]{vanBruxvoort_2021}.


\paragraph{Justice}
Justice can be defined as \textquote{the quality of being just, impartial, or fair} \parencite{dictionary_justice}. In the words of \textcite[p.~7]{Floridi_2019}, justice in an AI context means \textquote{eliminating unfair discrimination, promoting diversity, and preventing the rise of new threats to justice.} Once again following the categorization done by \textcite{Ryan_2021}, \textit{Justice} includes the subcategories Justice, Fairness, Consistency, Inclusion, Equality, Equity, Non-bias, Non-discrimination, Diversity, Plurality, Accessibility, Reversibility, Remedy, Redress, Challenge, and Access and distribution. Justice is used by 39 papers (81\%), and is thus the most used core principle among the reviewed papers.

Just AI systems should minimize the bias found in their data \parencite{Rothenberger_2019,Nauck_2019,Jobin_2019,Fjeld_2020,Clarke_2019}, be inclusive \parencite{Mikalef_2022,Lu_2022}, encourage diversity \parencite{Mikalef_2022} and equality \parencite{Gupta_2021,Fjeld_2020,Canca_2020}, and avoid discriminatory results against individuals or groups of individuals \parencite{Mikalef_2022,Werder_2022,Lu_2022,Jobin_2019,Hacker_2022}. Any negative consequences that cannot be avoided through non-maleficence should be distributed fairly, as should the benefits gained through beneficence \parencite{Jobin_2019,Doorn_2021}. AI developers should not only avoid previous bias, such as bias found in existing datasets, but also be aware of the risk of discriminating in new ways and on previously unforeseen scales \parencite[p.~48]{Fjeld_2020}.

To ensure AI systems are just, calls are once again made for involving stakeholders (e.g., \cite{Lu_2022,Fjeld_2020}), ensuring \textquote{voices and data of minority populations} are included in the AI system \parencite[p.~10]{Lukkien_2021}, and that potentially missed biases are mitigated \parencite{Siala_2022}. This can be strengthened by having diverse AI development teams, ensuring a wide range of genders, ages, races and cultural backgrounds \parencite[p.~104]{Lu_2022}, as well as professional backgrounds and skill sets \parencite[p.~52]{Fjeld_2020}. Just AI systems should ensure they are built on diverse \parencite{Doorn_2021} and representative \parencite{Hacker_2022,Fjeld_2020} data, and access to AI systems should be distributed equally and fairly \parencite{Jobin_2019,Fjeld_2020,Canca_2020}. Individuals and groups from worse-off backgrounds should be especially protected from harm \parencite{Canca_2020}.

Fairness can be controlled with risk identification methods and checklists specifically aimed at fairness \parencite[p.~103]{Lu_2022}. According to \textcite{Lu_2022}, there already exist technical tools, such as \textquote{IBM’s AI Fairness 360, Google’s Fairness Indicators, Microsoft’s Fairlearn, and UChicago’s Aequitas} (p.~107), and statistical methods, such as \textquote{demographic parity, data augmentation, weighted data sampling, re-sampling, re-weighting, swapping labels, removing dependencies, equalized odds checking} (p.~109) for assessing the fairness of a system. Organisations can then show the fairness of their systems by disclosing \textquote{summary statistics showing the distribution of scores between different protected groups} \parencite[p.~365]{Hacker_2022}.


\subsubsection{Instrumental principles}
\label{sec:results-principles-instrumental}
Whereas core principles are intrinsically valuable, instrumental principles provide ethical value by supporting and facilitating for the core principles \parencite{Canca_2020}. Which instrumental principles to include when developing a responsible AI system is therefore up to the developers and designers of the system. As ethics is not absolute, the selection of instrumental principles may vary depending on, amongst others, the cultural context of the AI system \parencite{vanBruxvoort_2021}, the support system of the country the AI system is to be used in \parencite{Wright_2018}, or the ethnicity, gender, political leaning and background of afflicted stakeholders \parencite{Jakesch_2022}, and may even change over time \parencite{vanBruxvoort_2021}.

Based on the clustering described above, the reviewed papers include the following categories of instrumental principles: \textit{Transparency}, \textit{Accountability}, \textit{Trust}, \textit{Sustainability}, \textit{Privacy} and \textit{Others}, where the latter encapsulates Laws and regulations, Predictability, Quality benchmarking, and Interventions and co-design. A complete overview of the instrumental principles used by the reviewed papers is shown in \autoref{tab:paper-principles-instrumental} (\autoref{app:principles}).

While this list encompasses instrumental principles used by the reviewed papers, it is by no means an exhaustive list of potential principles designers of AI systems can use, nor is that the intention of this section. Instead, this section is meant to give an overview of some principles that are used by current literature on responsible AI, in order to show some methods to facilitate for the core principles -- the principles that actually make an AI system responsible -- mentioned above. While the principles mentioned here are relatively common, and can thus work as a starting point for selecting relevant instrumental principles for a new system, designers should ensure they are adapted to the system and context they operate in, and that relevant principles that are not mentioned here are also included in their projects.


\paragraph{Transparency}
% Both system-, process- and technical
Transparency is defined by \textcite[p.~2]{Dignum_2017} as the ability to \textquote{[understand] the ways systems make decisions and how the data is being used, collected and governed.} Transparency is the most used principle among the reviewed principles, being used by 44 papers (92\%). According to \textcite{Ryan_2021}, \textit{Transparency} includes the subcategories Transparency, Explainability, Explicability, Understandability, Interpretability, Communication, Disclosure, and Showing.

Transparent AI systems should be able to explain their decisions in a way that is understandable for humans \parencite{Clarke_2019}, and adapted to the audience it is presented for \parencite{BarredoArrieta_2020,Hacker_2022}. It should be clear for users of an AI system that they are interacting with an artificial intelligence, and not a human being \parencite{Clarke_2019,Nauck_2019}, and AI systems should be open about how and why data is collected and used \parencite{Jobin_2019,Clarke_2019}. Not only should AI systems be transparent, but AI organisations should be open about the decisions taken during the development of the AI systems \parencite{Vakkuri_2022}, which protocols were used for development \parencite{vanBruxvoort_2021}, and what AI governance methods used to ensure that their system stays responsible \parencite{Dignum_2017,Dignum_2019}. Transparent AI systems should involve stakeholders in the decisions taken during the design of the system \parencite{Dignum_2019}. AI systems could design and incorporate metrics for stakeholder understanding of the system \parencite{Havrda_2020}, and track these similar to other key performance indicators. Some of the guidelines reviewed by \textcite{Jobin_2019,Fjeld_2020} argue that responsible AI systems should open-source their code and data, to fully facilitate public scrutiny.


Modern AI methods, such as deep neural networks and reinforcement learning systems, are complicated mathematical systems, whose decisions are typically hard to explain \parencite{BarredoArrieta_2020}. Models based on these methods are therefore often called black box models \parencite{Bélisle-Pipon_2022}. Several technical tools exist to make such black box models more transparent (see, e.g., \cite{BarredoArrieta_2020}), although these may lead to a trade-off, where increased transparency leads to worse performance \parencite{BarredoArrieta_2020}. Calls have therefore been made to abandon well-performing but hard-to-explain black box models, and instead adopt more interpretable models \parencite{Rudin_2019,Rizinski_2022,BarredoArrieta_2020}.

Transparency creates ethical value in two ways. First, it creates a way to audit the core principles \parencite{Canca_2020}. This enables both internal and external scrutiny of the system, which can decrease the time that passes before unforeseen harm, injustice or discrimination is detected. Similarly, this can ensure loss of autonomy is quickly noticed, thus decreasing the risk of it becoming a problem. Secondly, transparent systems allow developers to understand how their system works, and thus how it can be improved to provide better results \parencite{BarredoArrieta_2020}. This may help offset the trade-off mentioned earlier, thus increase the beneficence of the system.

Not all papers agree with transparency being an instrumental principle. \textcite{Floridi_2018}, and papers using their principles, include Explicability as part of their core principles, arguing that the other principles cannot be put into practice, and especially not evaluated, without having an explicable system. In a similar fashion, \textcite{Jobin_2019} discuss a claim by \textcite{Turilli_2009}, that \textquote{transparency is not an ethical principle in itself but a proethical condition for enabling or impairing other ethical practices or principles} \parencite[p.~105]{Turilli_2009}. As \textit{Transparency} can be enabled and used the same as other principles for responsible AI, it is still included in this paper, but this, combined with the broad usage of the principle among the reviewed papers, indicate that transparency should be strongly considered for all responsible AI systems, no matter the context they operate in.


\paragraph{Accountability}
Accountability is described as \textquote{the ability to determine whether a decision was made in accordance with procedural and substantive standards and to hold someone responsible if those standards are not met} \parencite[p.~36]{Anagnostou_2022}. 28 of the reviewed papers (58\%) include accountability as part of their principles, making it the second most used instrumental principle. According to the categories of \textcite{Ryan_2021}, \textit{Accountability} (which they describe as \textit{responsibility}) includes subcategories Responsibility, Accountability, Liability, and Acting with integrity.

Accountable AI systems should ensure a legal person, or group of legal persons, are held responsible for impacts of the automatic system \parencite{Werder_2022,Vakkuri_2022}, and for the different parts of the system \parencite{Lu_2022}. Such responsibility may be shared between developers and operators of the system \parencite{Vakkuri_2022}, and can be ensured by including a human in the loop, as discussed as a solution for \textit{Autonomy} \parencite{Doorn_2021}. In cases where unintended impacts happen, accountable AI systems should have ways to appeal the automatic decisions, and remedies for issues caused \parencite{Fjeld_2020,Eitel-Porter_2021,Clarke_2019}. In order to enable post hoc analyses of such incidents, decisions made during the development process should be documented and saved, to enable audits of the system \parencite{Rizinski_2022,Havrda_2020,BarredoArrieta_2020}. Such audits should be used to improve the system over time \parencite{Fjeld_2020}.

It is important that accountable AI systems have justifiable \parencite{Kumar_2021} and replicable \parencite{Fjeld_2020} outcomes, \textquote{based on original data} \parencite[p.~3]{Merhi_2022} and \textquote{derivable from [\dots] the learning algorithms used} \parencite[p.~5]{Dignum_2017}.  Accountability can be established through certification standards, which are popular among AI developers \parencite{Henriksen_2021}, or the use of contracts, especially useful for dividing responsibility between developers and customers of a system \parencite{Vakkuri_2022,Jobin_2019}. In order to be accountable, AI organisations should protect whistleblowers \parencite{Jobin_2019}, and have a way for workers to veto an AI system if there are any concerns \parencite{Anagnostou_2022}.

There has been some discussion regarding whether AI systems themselves can be held responsible for their actions, the same way legal persons are (e.g., papers reviewed by \cite{Jobin_2019}). The consensus, however, is that such responsibilities cannot be placed on the technology, but should instead be placed on the people designing, developing and operating the system \parencite{Fjeld_2020,Doorn_2021,Anagnostou_2022}.

Accountability is made possible by transparency, as decisions made during development of the system can be used to assign responsibility \parencite{Vakkuri_2022}. By enabling audits, accountability minimizes the risk of harm or discrimination going unnoticed, this supporting the core principles of \textit{Non-maleficence} and \textit{Justice}. Accountability further supports the notion of having a human in the loop, or otherwise responsible for decisions made automatically, thus supporting the principle of \textit{Autonomy}.


\paragraph{Trust}
Public trust in AI systems is essential if they are to reach their full potential \parencite{Jobin_2019}. Trust was included in 6 of the reviewed papers (13\%), including one \parencite{Jakesch_2022} who dropped it as a result of their pilot study. This leaves it as the least popular principle among the reviewed papers. According to \textcite{Ryan_2021}, \textit{Trust} only has one subcategory: Trustworthiness.

AI organisations should create trustworthy and reliable systems, and prove this to their stakeholders \parencite{Ryan_2021}. To create trust, systems should use high-quality data to work as intended and expected \parencite{Ryan_2021}, and to minimize bias \parencite{WangY_2020}. Organisations should facilitate for trust \textquote{among scientists and engineers,} in order to advance the state of AI research \parencite{Jobin_2019}.

Trust creates value by facilitating for accountability, as the two principles align closely -- increasing accountability is likely to increase trust, and increasing trust is likely to enable more feedback on systems, thus maximizing the potential for beneficence, while enabling early warnings of injustice and maleficence.


\paragraph{Sustainability}
Sustainability in an AI context means that the environmental impact of AI systems should be minimized \parencite{Siala_2022}. Sustainability was used in 7 of the reviewed papers (15\%). \textcite{Ryan_2021} give \textit{Sustainability} the subcategories Sustainability, Environment (nature), Energy, and Resources (energy).

Sustainable AI systems should avoid harming the environment more than necessary \parencite{Mikalef_2022}, and to \textquote{increase prosperity for [\dots] the planet} \parencite[p.~97534]{Rizinski_2022}. As AI models grow in size and depth, they should be \textquote{developed in an environmentally conscious manner} \parencite[p.~75]{Ryan_2021}, minimizing their energy consumption and environmental footprint \parencite{Ryan_2021,Jobin_2019}, such as by using \textquote{special hardware designed for energy-efficient learning} \parencite{vanBruxvoort_2021}. \textquote{AI should not be used to harm biodiversity} \parencite[p.~75]{Ryan_2021}, making it reasonable to consider the environment and animals (i.e., all \textquote{living beings} \parencite[p.~4]{Havrda_2020}) as stakeholders of the system.

Sustainability is largely aligned with non-maleficence and beneficence \parencite{Peters_2020}, in that positive outcomes of sustainability -- such as reduced harm to the environment -- leads to AI systems that do good and minimize harm. As \textquote{climate change [\dots] increasingly affect the poor} \parencite[p.~IX]{OECD_climate_change}, limiting this can also be seen as a contribution toward global justice.


\paragraph{Privacy}
The principle of Privacy in an AI context \textquote{relates to the use of data and the need to protect people's right to privacy} \parencite[p.~6]{Doorn_2021}, i.e., people's right to \textquote{freedom from unauthorized intrusion} \parencite{dictionary_privacy}. Privacy was used in 20 of the reviewed papers (42\%). According to \textcite{Ryan_2021}, \textit{Privacy} includes the subcategories Privacy, and Personal or private information.

Privacy in AI systems \textquote{includes both information provided by users and information generated about those users derived from their interactions with the system} \parencite[p.~106]{BarredoArrieta_2020}, and both direct (i.e., through access to the data) and indirect (i.e., through access to the model) information exposure \parencite[p.~1155]{Cheng_2021}. Privacy-compliant AI systems should collect as little data as possible while still fulfilling their goal \parencite{Doorn_2021,Havrda_2020,Ryan_2021}, and should only do so with prior consent from their data sources \parencite{Fjeld_2020,Rothenberger_2019}. Data should be stored safely \parencite{Liu_2021}, and personal data should be removable if the source of the data so wishes \parencite{Fjeld_2020,Ryan_2021}. AI organisations complying with the principle of privacy should conduct privacy-focused impact assessments \parencite{Havrda_2020}, and limit access to their data \parencite{Jobin_2019}. Finally, AI developers following the principle of privacy can implement a range of privacy-preserving technical tools, such as federated learning, decentralized learning \parencite{Lu_2022} or de-identification \parencite{Ryan_2021}, to increase the privacy of their system.

As the harm to avoid in Non-maleficence includes \textquote{violation of privacy} \parencite[p.~9]{Jobin_2019}, it is clear that increasing privacy directly leads to value through Non-maleficence. The same connection can be found with Autonomy, where increasing privacy leads to fewer connections between a person and their digital actions or expression, thus allowing freer speech and more individual freedom. As personal data such as race and gender are removed from AI systems, increasing privacy may also lead to less discrimination, thus creating value through increasing Justice.


\paragraph{Others}
Some reviewed papers highlight principles that do not fit within the definitions of the other clusters. As these are not used widely enough to be considered their own categories, they are instead described separately.

% Laws and regulations
\textit{Laws and regulations} is included by \textcite{Merhi_2022,Mikalef_2022} who argue that \textquote{AI systems should adhere to the respective laws and regulations that dictate their functioning} \parencite[p.~259]{Mikalef_2022}. This is especially important when AI is used to automate jobs or disrupting existing markets, where laws and regulations ensure the harm caused is as small as possible \parencite{Mikalef_2022}.

% Predictability
\textit{Predictability} is used by \textcite{Vakkuri_2022} to explain systems that \textquote{act in a predictable manner in any given situation} (p.~102). Increasing the predictability of an AI system can make it easier to understand how it works, and thus creates value through increasing the Transparency of the system -- with the increased value that brings \parencite{Vakkuri_2022}.

% Quality benchmarking
\textit{Quality benchmarking}, as described by \textcite{Hacker_2022}, has two benefits. First, disclosing performance metrics of AI systems would give potential customers ways to objectively compare different systems, thus creating competition in the AI market. Secondly, having quality benchmarks allows for better assessment of risk and rewards of AI systems, compared to non-AI systems. Both these benefits show that quality benchmarking creates value by increasing the Transparency of the system it is implemented in.

% Interventions and co-design
Finally, \textit{Interventions and co-design}, also used by \textcite{Hacker_2022}, includes taking existing methods for creating responsible systems, from fields such as human computer interaction and RRI, to design systems alongside stakeholders and affected persons. Doing so has the possibility of creating value through all core principles -- Autonomy and Non-maleficence can be supported by ensuring previously hidden harms and negative outcomes are detected early, Beneficence can be supported by ensuring alignment of stakeholder- and system values and priorities, and Justice can be supported by including minorities and specially affected persons among the involved stakeholders, thus designing a system that does not negatively impact those groups.


\subsubsection{Methods for enabling responsible AI}
While the principles discussed above contain some ways to implement them, some of the reviewed papers discuss overarching methods that can be used to implement responsibility as a whole to AI systems.

AI organisations looking to develop responsible AI systems should adopt a \textquote{learning governance model}, where decision-making is based on reflection and overseen by stakeholders \parencite{Morley_2021}. They should distribute responsibility for their systems on the development teams working on them, and create processes to hold these teams responsible for unintended occurrences within their field of responsibility \parencite{Rakova_2021}. In addition, these organisations should consider creating multi-disciplinary advisory boards for AI ethics \parencite{WangY_2020,Lu_2022}. Leaders of such organisations should work to show their commitment to responsible practices \parencite{Papagiannidis_2022}, while celebrating responsible successes \parencite{Lu_2022} and openly admitting failures \parencite{Rakova_2021} along the way. Responsible practice should be included as part of the evaluation of employees, to increase the personal incentives for developing responsible systems \parencite{Rakova_2021}. Firms looking to automate tasks should \textquote{consider the welfare of their employees, and how their actions may violate the norms associated with various social contracts} \parencite[p.~827]{Wright_2018}.

Many papers highlight the need for stakeholder involvement in order to develop responsible systems. \textcite{Buhmann_2021} call for using open-forum discussions, where information can be shared and all stakeholders, especially those at risk of negative consequences, can raise their concerns and present suggestions. Similarly, \textcite{Dignum_2019,Dignum_2017} highlights the need for education of stakeholders, if they are to fully participate in discussions. \textcite{Clarke_2019,Havrda_2020} argue that analysis of a wide range of stakeholders should form a basis for risk assessments of the system, with the intention of discovering risks that are important to mitigate. Similarly, \textcite{Dignum_2021} argue that stakeholders should be involved in decisions when models \textquote{use human data, affect human beings or have other morally significant impacts} (p.~4). \textcite{Gianni_2022} calls for including the public when defining \textquote{AI's role for our future societies} (p.~14). Finally, \textcite{Lukkien_2021} argue that minorities and people from different cultural background should be included in not only AI development, but also AI research.

\textcite[p.~104]{Lu_2022} call for including responsible AI principles in agile software development methods, such as sprint planning, testing and code reviews \parencite{Dybå_2008_agile}. While this requires some adaption, primarily in the form of implementing testing methods and metrics for measuring principle implementation, doing so ensures frequent testing, evaluation and monitoring of the responsibility of an AI system, thus increasing the probability of a successful adoption of responsibility \parencite{Jobin_2019,Rakova_2021,Nauck_2019}.

Non-functional requirements (NFR), such as usability, performance and security, are considered a critical component of many existing software development projects (see e.g., \cite{Chung_2009_NFR,Glinz_2007_NFR,Khatter_2013_NFR}). \textcite{Lu_2022} argue that many responsible AI principles, such as Fairness, Non-maleficence and Justice, can be implemented in the form of NFRs. This allows developers to implement responsible AI principles in existing systems with minimal adoption, thus lowering the barrier for making AI systems responsible.

Some of the reviewed papers argue that national governments should do more to ensure responsibility of AI systems operating within their countries. \textcite{Chen_2020,Brand_2022} call for national AI registers, where organisations must register any AI-powered systems they develop. These systems should undergo impact assessments, and the level of impact it may have on humans decides the requirements of the system \parencite{Brand_2022}. For example, a system with significant impact may be required to implement "safeguards", systems that report on any errors or imbalances in the AI system \parencite{Chen_2020}. Reports are then sent regularly to the government, to facilitate auditing of the systems \parencite{Brand_2022,Chen_2020}. \textcite{Floridi_2018} argue that the IT infrastructure of the justice system should be improved, in order to enable auditing of AI systems in court.



\subsection{RQ2 -- Antecedents for responsible AI}
\label{sec:results-rq2:antecedents}
The antecedents for responsible AI  presented by the reviewed papers were clustered using an open coding-approach. This is described by \textcite{Corbin_1990_opencoding} as a method suitable for creating new insights, by clustering qualitative data without pre-defined categories. This method led to three clusters of antecedents: Laws and regulations, Stakeholder demands and loss of reputation, and Internal motivation.

\paragraph{Laws and regulations}
Laws and regulations appear to be the most common antecedent mentioned among the reviewed papers. The European Union's General Data Protection Regulation (GDPR) is a European law enforcing privacy rights \parencite{Chen_2020,Werder_2022,Nevanperä_2021}, giving European stakeholders the right to control which personal data can be used for decisions in AI systems \parencite{Hacker_2022}, and a right to have personal data fully erased, if they so want \parencite{Fjeld_2020}. GDPR Article 13 further provides European citizens a right to \textquote{meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing for the data subject} \parencite[p.~41]{GDPR}, if they are the subject of automatic processing. While it does not involve the core principles directly, GDPR operates as an antecedent for the principles of Privacy and Transparency, requiring them for any AI system operating within the European Union. In addition, both Product Liability Law and Contract and Tort Law requires developers targeting the European Union to provide Transparency if doing so reduces risk more than it reduces the utility of the system, and Banking Law requires AI systems used by banks to be Transparent \parencite{Hacker_2022}.

In addition to the European-based GDPR, other laws may require similar principles. In the US, the Health Insurance Portability and Accountability Act (HIPAA) protects personal information in a healthcare context, thus working as an antecedent for Non-maleficence for AI systems operating in US healthcare \parencite{Werder_2022}. Similar protections are required under the Act on the Protection of Personal Information (APPI) in Japan, thus increasing Non-maleficence for AI systems on the island \parencite{WangY_2020}. In Norway, the protections of GDPR are provided by \textcite{personvernforordningen}, providing the same benefits to responsibility. Other places may have local regulations that include \textquote{principles of standards and ethical considerations, such as auditing processes and algorithmic impact assessments} \parencite[p.~60-61]{Papagiannidis_2022}, thus enabling several of the principles and implementation methods described in \autoref{sec:results-rq1}.

None of the laws mentioned above directly affect AI systems, instead placing limitations on data usage and Transparency. The European Union has, however, recently proposed an Artificial Intelligence Act (AIA), that specifically targets AI systems, banning systems with unacceptable risk and placing strict requirements on high-risk systems \parencite{AIA}. If AIA is implemented, stricter regulations are likely to be placed on AI systems operating in the European Union, thus increasing the requirements of responsibility affecting these systems \parencite{Hacker_2022}.

There are multiple potential reasons why Laws and regulations are the most common antecedent among the reviewed papers. \textcite{Rakova_2021} describe organisations as reactive, thus only acting when they are forced to do so by external factors, such as binding laws. At the same time, breaking AI regulation comes with potentially large economic costs, with severe infringements potentially resulting in a fine of 4\% of the organisation's worldwide annual revenue \parencite{GDPR_fines}. This provides a quantitative measure of responsibility of AI systems, lowering the economic benefit of developing irresponsible models and justifying the additional resources required for implementing principles \parencite{Morley_2021}.

Although laws and regulations help increase the responsibility of AI systems, there are some who criticise the current state of AI regulations. \textcite{Chen_2020} states that GDPR only protects personal data, and thus does not protect users when no personal data is used. Developers interviewed by \textcite{Henriksen_2021} are worried that principles for responsible AI may delay new regulations that could provide clearer and stricter regulations on AI systems. Similar worries are also noted by \textcite{Hagendorff_2020}. Finally, \textcite[p.~694]{Floridi_2018} argue that \textquote{compliance with the law is merely necessary [\dots], but significantly insufficient,} and that AI organisations therefore must do more than simply comply with existing regulations.


\paragraph{Stakeholder demands and loss of reputation}
\textcite{Buhmann_2021} propose that stakeholders are essential for AI systems to operate, and that organisations that struggle to explain their systems when key stakeholders demand such information, risk their reputation. Similar concerns of reputation are echoed by \textcite{Havrda_2020,Eitel-Porter_2021}, showing the need for Transparency in AI systems. This need is further strengthened by the hidden inner workings typical for AI systems scaring stakeholders from engaging with the technology \parencite{Merhi_2022}. Adding on this, \textcite[p.~1]{Clarke_2019} argue that \textquote{it is in the interest of all organisations to avoid their stakeholders suffering harm,} and that such suffering can be only be minimized by making sure AI systems follow the principle of Non-maleficence. As reputation losses bring with them the potential of significant economic losses, this can, like with regulation, be quantified and used to justify the cost of adhering to responsible principles \parencite{Hagendorff_2020}.

When providing AI systems to professional organisations, such organisations may have their own codes of ethics, which requires responsibility from their suppliers \parencite{Anagnostou_2022}. In such cases, having an irresponsible AI system may lead to loss of sales. As markets are \textquote{visible, dynamic and viable,} such requirements make responsibility in AI systems a \textquote{must-have requirement for success} \parencite[p.~2]{Gupta_2021}. Similar expectations may, over time, become apparent also with non-company stakeholders and customers \parencite{WangY_2020}.

Although several papers argue why stakeholder demands work as antecedents for responsible AI, \textcite[p.~8]{Morley_2021} observe that while laws and regulations actually lead to changed design practices, \textquote{other benefits, such as those related to public reputation and consumer loyalty, seem to motivate public declarations of compliance with principles, but do not yet provide sufficient motivation for altering design behaviours or practice.} \textcite{Rakova_2021} found that organisations are reactive, and thus are only driven to act when the risk of catastrophic media attention is too high.


\paragraph{Internal motivation}
\textcite{Kumar_2021} state that internal motivation, together with regulations, are the primary antecedents for responsible AI development. \textcite{Henriksen_2021} found that the AI developers they interviewed were motivated by standards and certifications, rather than principles. In a similar fashion, \textcite{Rakova_2021} found that most current work on responsible AI in organisations was driven by individual workers and their voluntarily sacrificed resources. As human nature is inherently to be selfless and do good \parencite{Ward_2012_results}, it is natural to assume that most AI workers want to do good for humanity and create responsible AI systems.


\subsubsection{Barriers preventing responsible AI}
\label{sec:results-barriers}
To understand factors that prevent AI from becoming responsible, the same approach was conducted on barriers presented in the reviewed papers. Once again, three clusters were discovered -- Abstract principles, Opaque systems and Toothless guidelines.

\paragraph{Abstract principles}
One core barrier presented by the reviewed papers is the abstractness of responsible AI principles, and lack of methods for implementing the details in practice (see e.g. \cite{Jakesch_2022,Havrda_2020,BarredoArrieta_2020,Lukkien_2021}). \textcite{Gianni_2022} argue that there are \textquote{difficulties in translating principles and operational normative tools, such as transparency or fairness, into concrete measures} (p.~9), although, according to \citeauthor{Miller_2019_techworker} (\citeyear{Miller_2019_techworker}; as cited by \cite{Morley_2020}), 78\% of tech workers want practical tools for evaluate ethical impacts of their products. \textcite{Morley_2021} find that AI practitioners need assistance that \textquote{go beyond general guidelines for professional and ethical practice}, and instead is \textquote{practically applicable to real-world ethical decisions} (p.~2). Multiple papers (e.g., \cite{Chen_2020,BarredoArrieta_2020,Rakova_2021}) note that there is a gap between what is being researched in academia and what is actually implemented in organisations.

% This mirrors the field of philosophy, where there is a debate on whether \textquote{principlism} can replace moral rules and impact actions (see, e.g., \cite{Clouser_1990_principlism,Lustig_1992_principlism,Jones_2020_principlism}).

\paragraph{Opaque systems}
Another barrier comes as a result of AI technology often being proprietary, limiting the information organisations are willing to share about their system \parencite{Buhmann_2021,Hacker_2022}. Although calls have been made for sharing the data and source code used to develop systems, doing so may not lead to public understanding of how the system works, due to the opacity of prevalent AI methods \parencite{Buhmann_2021,Siala_2022}. At the same time, increasing transparency may increase the risk of hacking, thus limiting the Non-maleficence of the system \parencite{Cheng_2021}.

\paragraph{Toothless guidelines}
One issue that ethical guidelines are considered toothless \parencite{Henriksen_2021}, and there is a lack of ways to easily enforce them \parencite{Liu_2021,Hagendorff_2020}. As long as laws do not regulate AI systems in detail, this leaves stakeholders with few options for remedy against irresponsible AI systems \parencite{Henriksen_2021}. \textcite{McNamara_2018_results} (cited in \cite{Nevanperä_2021}) found almost no effect from ethical guidelines on the work of software developers. Similar results are found by \textcite{Hagendorff_2020}. This barrier is further strengthened by companies who believe that following laws and regulations are sufficient for having ethical systems \parencite{Vakkuri_2022}. 


\subsection{RQ3 -- Business advantages of responsible AI}
\label{sec:results-rq3:advantages}
To understand the advantages businesses may gain from making their AI systems responsible -- and thus answer RQ3 -- open coding was once again used on the outcomes and business advantages presented in the reviewed papers. This led to three clusters of business advantages -- Increased reputation and stakeholder trust, Long-term profitability, and Internal improvements.

\paragraph{Increased reputation and stakeholder trust}
Most organisations looking to make their AI systems responsible primarily do so to minimize risks \parencite{Cheng_2021,Eitel-Porter_2021}, as irresponsible use of AI can lead to severe harm to an organisation's reputation \parencite{Siala_2022,Hagendorff_2020}. Implementing responsibility in AI systems means -- by definition -- that irresponsible use of AI is reduced, and can thus both protect responsible AI organisations from reputational harms \parencite{Buhmann_2021}, and increase their reputation among stakeholders \parencite{Werder_2022}.

\textcite{Anagnostou_2022,Gupta_2021,Merhi_2022} all discuss that AI systems fulfilling the principles of Transparency, where decisions can be explained and discussed, and Justice, where systems avoid bias, are likely to lead to increased trust from customers and stakeholders, as decisions are fair and understandable for those affected by them. Similarly, ensuring AI systems follow the principle of Transparency enables external reviews and audits \parencite{Hacker_2022,Rizinski_2022}, which again increases stakeholder trust in the system \parencite{reinoso_2021_audits}. This trust can be further strengthened by being able to test and directly compare AI systems against traditional solutions, as suggested via the principle of Quality benchmarking proposed by \textcite{Hacker_2022}. Ensuring that AI systems are beneficent, and clearly showing the beneficence of the system, is likely to lead to improved acceptance, and thus use, of an AI system \parencite{Liu_2021,vanBruxvoort_2021}. In a similar fashion, \textcite{WangW_2021} found that following the principles of Autonomy, Transparency, Justice, Beneficence and Non-maleficence increased use of, and satisfaction with, AI systems in a healthcare context.


\paragraph{Long-term profitability}
\textcite{Buhmann_2021} argue that by engaging directly with a wide range of stakeholders, organisations can profit by quickly gathering new information, such as changing interests or reactions of stakeholders to new systems, that can give them an advantage compared to organisations that lack direct access to their stakeholders. Similarly, \textcite{Zaheer_2017_trustvalue} (referenced in \cite{Liu_2021}) found that creating trust among stakeholders will increase their willingness to share information. \textcite{Liu_2021,Morley_2021} found that responsible AI methods, through creating trust, sets up long-term relationships with customers, thus increasing customer loyalty. As trust has an economic value \parencite{Minkkinen_2021}, increasing trust indirectly increases the profitability of a firm. Profitability can be further strengthened by responsible AI methods helping organisations avoid or minimize costly mistakes \parencite{Floridi_2018,Rakova_2021}.

Interviews conducted by \textcite{Kumar_2021} found that responsible methods increased users' perceived value of AI systems, which again increases the value of the system, suggesting that responsible AI systems indirectly leads to increased market share for AI organisations. Other papers repeat the connections between responsible AI and market share \parencite{Cheng_2021} or organisational performance \parencite{Werder_2022}. Implementing responsibility in AI systems may also increase the brand value among customers looking for ethical solutions to their problems \parencite{Minkkinen_2021,WangY_2020}.

\paragraph{Internal improvements}
Adopting responsible AI practices may lead to improvements within the organisations, that can create long-lasting business advantages. \textcite{WidénWulff_2004_trustvalue} (referenced in \cite{Liu_2021}) argue that trust, and other similar values, can lead to corporate well-being and innovativeness. A shortage of qualified developers may give organisations committed to responsible AI an edge when it comes to recruiting new talent \parencite{Papagiannidis_2022}. By interviewing Nordic companies working with AI, \textcite[p.~65]{Papagiannidis_2022} found that adopting responsible AI practices positively influenced the knowledge management capabilities of an organisation, which again \textquote{positively influenced competitive performance.}
